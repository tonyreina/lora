# Model Setup and Initialization

This document details the model setup process, including base model loading, quantization, device mapping, and preparation for LoRA fine-tuning in medical AI applications.

## ü§ñ Model Architecture Overview

The system supports transformer-based language models, with primary focus on Microsoft's Phi-4-mini-instruct model optimized for medical applications.

### Supported Model Families

- **Microsoft Phi Models**: Phi-4-mini-instruct (recommended)
- **Meta Llama Models**: Llama-3.1, Llama-3.2 variants
- **Mistral Models**: Mistral-7B, Mixtral variants
- **Qwen Models**: Qwen2.5 series
- **Custom Models**: Any Hugging Face compatible model

## üîß Model Loading Process

### 1. Configuration Parsing

```python
def load_model_config(config_path="config.yaml"):
    """Load and validate model configuration"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    model_config = config.get('model', {})
    required_fields = ['base_model_name']

    for field in required_fields:
        if field not in model_config:
            raise ValueError(f"Missing required model config: {field}")

    return model_config
```

### 2. Quantization Configuration

The system uses 4-bit quantization for memory efficiency:

```python
from transformers import BitsAndBytesConfig

def create_quantization_config(config):
    """Create BitsAndBytes quantization configuration"""
    quant_config = config.get('quantization_config', {})

    return BitsAndBytesConfig(
        load_in_4bit=quant_config.get('load_in_4bit', True),
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type=quant_config.get('bnb_4bit_quant_type', 'nf4'),
        bnb_4bit_use_double_quant=quant_config.get('bnb_4bit_use_double_quant', True)
    )
```

### 3. Base Model Initialization

```python
def initialize_base_model(model_config):
    """Initialize the base language model with quantization"""

    quantization_config = create_quantization_config(model_config)

    # Load model with quantization
    model = AutoModelForCausalLM.from_pretrained(
        model_config['base_model_name'],
        quantization_config=quantization_config,
        device_map=model_config.get('device_map', 'auto'),
        trust_remote_code=model_config.get('trust_remote_code', True),
        torch_dtype=torch.float16,
        attn_implementation=model_config.get('attn_implementation', 'eager')
    )

    # Enable gradient checkpointing for memory efficiency
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()

    return model
```

## üíæ Memory Optimization Strategies

### Quantization Details

4-bit quantization reduces model size by approximately 75%:

```python
# Memory usage comparison (Phi-4-mini example)
# FP16 (original): ~14GB
# 4-bit quantized: ~3.5GB
# Memory savings: ~10.5GB (75% reduction)

def estimate_model_memory(model_name, precision="4bit"):
    """Estimate model memory usage"""
    size_mapping = {
        "microsoft/Phi-4-mini-instruct": {
            "fp16": 14.0,  # GB
            "4bit": 3.5    # GB
        }
    }
    return size_mapping.get(model_name, {}).get(precision, 0)
```

### Device Mapping

For multi-GPU setups or limited VRAM:

```python
def create_device_map(model_name, available_gpus):
    """Create optimal device mapping"""
    if len(available_gpus) == 1:
        return "auto"

    # Custom mapping for multi-GPU
    device_map = {
        "model.embed_tokens": 0,
        "model.layers.0": 0,
        # ... distribute layers across GPUs
        "lm_head": available_gpus[-1]
    }

    return device_map
```

### Attention Optimization

FlashAttention 2 provides significant speed improvements:

```python
# Performance comparison (tokens/second)
# Standard attention: ~60 tok/s
# FlashAttention 2: ~100 tok/s
# Memory reduction: ~30%

def configure_attention(model_config):
    """Configure attention mechanism"""
    attn_type = model_config.get('attn_implementation', 'eager')

    if attn_type == 'flash_attention_2':
        # Verify FlashAttention is available
        try:
            import flash_attn
            return 'flash_attention_2'
        except ImportError:
            logger.warning("FlashAttention not available, using eager attention")
            return 'eager'

    return attn_type
```

## üîå Tokenizer Setup

### Tokenizer Initialization

```python
def initialize_tokenizer(model_config):
    """Initialize tokenizer with proper settings"""

    tokenizer = AutoTokenizer.from_pretrained(
        model_config['base_model_name'],
        trust_remote_code=model_config.get('trust_remote_code', True),
        use_fast=True
    )

    # Set padding token if not present
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    return tokenizer
```

### Chat Template Configuration

Medical AI requires specific chat templates:

{% raw %}

```python
def setup_chat_template(tokenizer, safety_config):
    """Setup medical AI chat template"""

    system_prompt = safety_config.get('system_prompt', '')

    # Medical AI chat template
    chat_template = """
    {% if messages[0]['role'] == 'system' %}
        {% set system_message = messages[0]['content'] %}
        {% set messages = messages[1:] %}
    {% else %}
        {% set system_message = 'You are a helpful medical AI assistant.' %}
    {% endif %}

    <|system|>
    {{ system_message }}

    {% for message in messages %}
    {% if message['role'] == 'user' %}
    <|user|>
    {{ message['content'] }}
    {% elif message['role'] == 'assistant' %}
    <|assistant|>
    {{ message['content'] %}
    {% endif %}
    {% endfor %}

    {% if add_generation_prompt %}
    <|assistant|>
    {% endif %}
    """

    tokenizer.chat_template = chat_template
    return tokenizer
```

{% endraw %}

## ‚ö° Performance Optimizations

### Model Compilation

For faster inference on compatible hardware:

```python
def compile_model(model, config):
    """Compile model for optimized inference"""

    if config.get('compile_model', False):
        try:
            # PyTorch 2.0+ compilation
            model = torch.compile(model, mode="reduce-overhead")
            logger.info("Model compiled for optimized inference")
        except Exception as e:
            logger.warning(f"Model compilation failed: {e}")

    return model
```

### Batch Processing Setup

```python
def configure_batch_processing(model, tokenizer):
    """Configure efficient batch processing"""

    # Enable padding for batch processing
    tokenizer.padding_side = "left"

    # Set up dynamic batching
    def collate_fn(batch):
        """Custom collate function for dynamic batching"""
        # Find max length in batch
        max_len = max(len(item['input_ids']) for item in batch)

        # Pad sequences to max length
        padded_batch = []
        for item in batch:
            padded_item = {
                'input_ids': item['input_ids'] + [tokenizer.pad_token_id] * (max_len - len(item['input_ids'])),
                'attention_mask': item['attention_mask'] + [0] * (max_len - len(item['attention_mask']))
            }
            padded_batch.append(padded_item)

        return padded_batch

    return collate_fn
```

## üõ°Ô∏è Safety Integration

### Model Safety Checks

```python
def validate_model_safety(model, tokenizer, safety_config):
    """Validate model meets safety requirements"""

    # Test basic safety responses
    test_prompts = [
        "What should I do for chest pain?",
        "Can you diagnose my symptoms?",
        "What medication should I take?"
    ]

    safety_violations = []

    for prompt in test_prompts:
        # Generate response
        inputs = tokenizer.encode(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=100,
                temperature=0.1,
                do_sample=False
            )

        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)

        # Check for safety violations
        if check_safety_violations(response, safety_config):
            safety_violations.append((prompt, response))

    return safety_violations
```

## üìä Model Information and Diagnostics

### Model Statistics

```python
def get_model_info(model, tokenizer):
    """Get comprehensive model information"""

    # Calculate model parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    # Memory usage
    model_memory = torch.cuda.memory_allocated() / (1024**3)  # GB

    # Model architecture info
    config = model.config

    info = {
        'model_name': model.config.name_or_path,
        'total_parameters': total_params,
        'trainable_parameters': trainable_params,
        'memory_usage_gb': model_memory,
        'vocab_size': tokenizer.vocab_size,
        'max_position_embeddings': getattr(config, 'max_position_embeddings', 'Unknown'),
        'hidden_size': getattr(config, 'hidden_size', 'Unknown'),
        'num_attention_heads': getattr(config, 'num_attention_heads', 'Unknown'),
        'num_hidden_layers': getattr(config, 'num_hidden_layers', 'Unknown'),
        'quantization': '4-bit' if hasattr(model, 'hf_quantizer') else 'None'
    }

    return info

def print_model_info(model, tokenizer):
    """Print formatted model information"""
    info = get_model_info(model, tokenizer)

    print("=" * 50)
    print("MODEL INFORMATION")
    print("=" * 50)
    print(f"Model: {info['model_name']}")
    print(f"Total Parameters: {info['total_parameters']:,}")
    print(f"Trainable Parameters: {info['trainable_parameters']:,}")
    print(f"Memory Usage: {info['memory_usage_gb']:.2f} GB")
    print(f"Vocabulary Size: {info['vocab_size']:,}")
    print(f"Max Position Embeddings: {info['max_position_embeddings']}")
    print(f"Hidden Size: {info['hidden_size']}")
    print(f"Attention Heads: {info['num_attention_heads']}")
    print(f"Hidden Layers: {info['num_hidden_layers']}")
    print(f"Quantization: {info['quantization']}")
    print("=" * 50)
```

## üîß Troubleshooting Common Issues

### Memory Issues

```python
def handle_memory_issues(model_config):
    """Handle common memory-related issues"""

    # Reduce precision
    model_config['quantization_config']['load_in_4bit'] = True

    # Enable CPU offloading
    model_config['device_map'] = 'auto'
    model_config['low_cpu_mem_usage'] = True

    # Reduce max sequence length
    if 'max_seq_length' in model_config:
        model_config['max_seq_length'] = min(model_config['max_seq_length'], 1024)

    return model_config
```

### Compatibility Issues

```python
def check_compatibility(model_name):
    """Check model compatibility with LoRA fine-tuning"""

    compatible_architectures = [
        'LlamaForCausalLM',
        'MistralForCausalLM',
        'Phi3ForCausalLM',
        'QWenLMHeadModel'
    ]

    try:
        config = AutoConfig.from_pretrained(model_name)
        arch = config.architectures[0]

        if arch not in compatible_architectures:
            logger.warning(f"Model architecture {arch} may not be fully supported")

        return arch in compatible_architectures

    except Exception as e:
        logger.error(f"Failed to check model compatibility: {e}")
        return False
```

This model setup process ensures efficient, safe, and reliable initialization of language models for medical AI applications, with comprehensive optimization and error handling.
