#!/usr/bin/env python3
"""Pipeline functions for medical LLM training and inference."""

import gc
import os
from typing import Optional
import torch
from omegaconf import DictConfig

from src.data_utils import load_medical_dataset, prepare_datasets
from src.model_utils import (
    setup_model_and_tokenizer, setup_lora_model, create_trainer,
    save_model, evaluate_model
)
from src.inference_utils import load_inference_model, run_inference


def cleanup_memory():
    """Clean up GPU memory."""
    gc.collect()
    torch.cuda.empty_cache()
    print("âœ… Memory cleaned up")


def run_training(cfg: DictConfig) -> str:
    """Run the training pipeline."""
    print("ðŸ“š Starting training pipeline...")
    
    # Load and prepare data
    print("ðŸ“š Loading dataset...")
    raw_dataset = load_medical_dataset(cfg.data.train_file)
    
    # Setup model and tokenizer
    print("âš™ï¸ Setting up model and tokenizer...")
    model, tokenizer = setup_model_and_tokenizer(cfg.model.name, cfg.seed)
    
    # Configure LoRA
    print("ðŸ”§ Configuring LoRA...")
    model = setup_lora_model(model)
    
    # Prepare datasets
    print("ðŸ“Š Preparing datasets...")
    train_dataset, eval_dataset, test_dataset = prepare_datasets(
        raw_dataset, 
        tokenizer, 
        cfg.data.preprocessing.system_prompt,
        cfg.model.max_length
    )
    
    # Create trainer
    print("ðŸƒ Creating trainer...")
    trainer = create_trainer(
        model, 
        tokenizer, 
        train_dataset, 
        eval_dataset, 
        cfg.output_dir,
        cfg.training.batch_size, 
        cfg.training.learning_rate, 
        cfg.training.max_steps, 
        cfg.training.validate_steps
    )
    
    # Train model
    if cfg.training.early_stopping.enabled:
        patience = cfg.training.early_stopping.get('patience', 3)
        print(f"\nðŸš€ Starting training with early stopping (patience={patience})...")
    else:
        print("\nðŸš€ Starting training (no early stopping)...")
    trainer.train()
    print("\n\nFinished training.\n\n")
    
    # Save model
    print("ðŸ’¾ Saving model...")
    adapter_dir = save_model(model, tokenizer, cfg.output_dir)
    
    # Evaluate on test set
    print("ðŸ§ª Evaluating on test set...")
    test_results = evaluate_model(trainer, test_dataset)
    
    # Clean up memory
    cleanup_memory()
    
    print(f"âœ… Training complete! Adapter saved to: {adapter_dir}")
    return adapter_dir


def run_interactive_inference(cfg: DictConfig) -> None:
    """Run interactive inference."""
    print("ðŸ¤– Starting inference pipeline...")
    
    adapter_dir = cfg.inference.adapter_path
    
    if not os.path.exists(adapter_dir):
        print(f"âŒ Adapter directory not found: {adapter_dir}")
        print("Please run training first to create the adapter.")
        return
    
    print("ðŸ”„ Loading inference model...")
    model, tokenizer = load_inference_model(cfg.model.name, adapter_dir)
    
    print("ðŸ¤– Model loaded successfully!")
    
    if not cfg.inference.interactive:
        print("Non-interactive inference mode - exiting")
        return
    
    print("\n" + "="*60)
    print("Medical AI Assistant - Interactive Mode")
    print("Type 'quit' or 'exit' to stop")
    print("="*60 + "\n")
    
    # Interactive loop
    while True:
        try:
            user_query = input("ðŸ‘¨â€âš•ï¸ Ask a medical question: ").strip()
            
            if user_query.lower() in ['quit', 'exit', 'q']:
                break
            
            if not user_query:
                continue
            
            print("\n" + "="*60)
            run_inference(
                model, 
                tokenizer, 
                user_query, 
                cfg.data.preprocessing.system_prompt
            )
            print("\n" + "="*60 + "\n")
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"âŒ Error: {e}")
            continue
    
    print("\nðŸ‘‹ Thank you for using the Medical AI Assistant!")


def run_demo(cfg: DictConfig) -> None:
    """Run demo inference with a sample question."""
    print("ðŸ¤– Starting demo inference...")
    
    adapter_dir = cfg.inference.adapter_path
    
    if not os.path.exists(adapter_dir):
        print(f"âŒ Adapter directory not found: {adapter_dir}")
        print("Please run training first to create the adapter.")
        return
    
    print("ðŸ”„ Loading inference model...")
    model, tokenizer = load_inference_model(cfg.model.name, adapter_dir)
    
    # Demo question
    demo_question = cfg.inference.demo_question
    
    print("ðŸ¤– Running demo inference...")
    print(f"Demo Question: {demo_question}\n")
    
    run_inference(
        model, 
        tokenizer, 
        demo_question, 
        cfg.data.preprocessing.system_prompt
    )
    
    print("\nâœ… Demo complete!")


def run_full_pipeline(cfg: DictConfig) -> None:
    """Run complete training and inference pipeline."""
    print("ðŸš€ Starting complete Medical LLM Training and Inference Pipeline")
    print("="*60)
    
    # Step 1: Training
    print("\nðŸ“š Step 1: Training the model...")
    adapter_dir = run_training(cfg)
    
    # Step 2: Demo inference
    print("\nðŸ¤– Step 2: Running demo inference...")
    run_demo(cfg)
    
    print("\nâœ… Complete pipeline finished!")
    print(f"Adapter saved to: {adapter_dir}")
    print("\nTo run interactive inference, use:")
    print("python main.py mode=inference")