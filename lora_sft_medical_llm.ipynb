{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, EarlyStoppingCallback, BitsAndBytesConfig\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "DATA_FILENAME = \"./data/my_custom_data.jsonl\"\n",
    "\n",
    "models = {\"phi\": \"microsoft/Phi-4-mini-instruct\", # https://huggingface.co/microsoft/Phi-4-mini-instruct\n",
    "          \"smol-135M\": \"HuggingFaceTB/SmolLM-135M-Instruct\", # https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct\n",
    "          \"smol-3B\": \"HuggingFaceTB/SmolLM3-3B\", # https://huggingface.co/HuggingFaceTB/SmolLM3-3B\n",
    "}\n",
    "MODEL_NAME = models[\"smol-135M\"]\n",
    "\n",
    "# Clear any cached memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b006704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path: str, cfg, seed: int = 816):\n",
    "    \"\"\"Load dataset and prepare for training with TRL-compatible conversational format.\"\"\"\n",
    "    \n",
    "    # Load raw data\n",
    "    raw_dataset = load_dataset(\"json\", data_files=file_path)[\"train\"]\n",
    "    \n",
    "    # Convert to conversational format that SFTTrainer expects\n",
    "    def format_to_messages(example):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": cfg.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
    "        ]\n",
    "        return {\"messages\": messages}\n",
    "    \n",
    "    # Apply the formatting\n",
    "    dataset = raw_dataset.map(format_to_messages, remove_columns=raw_dataset.column_names)\n",
    "    \n",
    "    # Split data\n",
    "    test_size = cfg.test_split + cfg.validation_split\n",
    "    val_ratio = cfg.validation_split / test_size\n",
    "    train_val = dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "    val_test = train_val[\"test\"].train_test_split(test_size=val_ratio, seed=seed)\n",
    "    \n",
    "    final_dataset = DatasetDict({\n",
    "        \"train\": train_val[\"train\"],\n",
    "        \"validation\": val_test[\"train\"],  \n",
    "        \"test\": val_test[\"test\"]\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"Train: {len(final_dataset['train'])}, Val: {len(final_dataset['validation'])}, Test: {len(final_dataset['test'])}\")\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c5a7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927e598e89fb453da1954c0a1be07f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9c3dda0709457e85bfd0a844e9508b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0bcdbc41dc4397956ff4b8e4076278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07128be24964b529cd96047a3a53250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e63491e4664bb6bf5f155a9769e9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/565 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-04 23:01:35.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_and_prepare_data\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mTrain: 80, Val: 10, Test: 10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.seed = 816\n",
    "        self.system_prompt = (\n",
    "            \"You are a careful medical assistant providing evidence-based information. \"\n",
    "            \"Always end with 'This response was generated by AI. Please check with medical practitioners.'\"\n",
    "        )\n",
    "        \n",
    "        # Data settings\n",
    "        self.data = type('DataConfig', (), {\n",
    "            'train_file': './data/my_custom_data.jsonl',\n",
    "            'test_split': 0.1,\n",
    "            'validation_split': 0.1,\n",
    "            'system_prompt': self.system_prompt\n",
    "        })()\n",
    "\n",
    "        self.output_directory = \"my_great_llm_model\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load and prepare the dataset in the format SFTTrainer expects\n",
    "dataset = load_and_prepare_data(DATA_FILENAME, cfg.data, cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f84d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PEFT with LoRA - optimized for better generalization\n",
    "peft_config = LoraConfig(\n",
    "    r=32,                         # Increased LoRA rank for better capacity \n",
    "    lora_alpha=64,               # Increased scaling factor proportionally \n",
    "    lora_dropout=0.1,            # Increased dropout for better regularization \n",
    "    bias=\"none\",                 # Bias training strategy\n",
    "    task_type=\"CAUSAL_LM\",       # Task type for causal language modeling\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # Added more target modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a54aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure BitsAndBytes for memory-efficient quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Use nf4 quantization type (recommended)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Computation dtype\n",
    "    bnb_4bit_use_double_quant=True,      # Use double quantization for even better memory savings\n",
    "    #load_in_8bit=True,                 # Alternative: use 8-bit instead of 4-bit (uncomment if preferred)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50461236",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    model_init_kwargs={\n",
    "        \"quantization_config\": bnb_config,   # Add BitsAndBytes quantization\n",
    "        \"device_map\": None,                  # Let the trainer handle device placement\n",
    "        \"attn_implementation\": \"flash_attention_2\",  # Enable flash attention for packing compatibility\n",
    "        \"dtype\": torch.bfloat16,       # Ensure consistent dtype for flash attention\n",
    "    },\n",
    "    packing=True,\n",
    "    num_train_epochs=100,               # Reduced from 100 to prevent overfitting\n",
    "    learning_rate=5e-5,               # Reduced learning rate (was 2e-4)\n",
    "    lr_scheduler_type=\"cosine\",       # Cosine annealing for better convergence\n",
    "    warmup_ratio=0.1,                 # Warmup for 10% of training steps\n",
    "    weight_decay=0.01,                # Added weight decay for regularization\n",
    "    loss_type=\"dft\",                  # Dynamic fine tuning to scale loss by probability of token (https://arxiv.org/pdf/2508.05629)\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=cfg.output_directory, \n",
    "    max_length=1024,\n",
    "    bf16=True,                             # Enable bfloat16 training for consistency with flash attention\n",
    "    dataloader_pin_memory=False,           # Disable pin memory to avoid dtype conflicts\n",
    "    \n",
    "    # Evaluation configuration\n",
    "    eval_strategy=\"steps\",             # Evaluate every eval_steps\n",
    "    eval_steps=25,                     # More frequent evaluation (was 50)\n",
    "    save_strategy=\"steps\",             # Save model every save_steps\n",
    "    save_steps=25,                     # More frequent saves (was 50)\n",
    "    logging_steps=5,                   # More frequent logging (was 10)\n",
    "    load_best_model_at_end=True,       # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\", # Use validation loss to determine best model\n",
    "    greater_is_better=False,           # Lower loss is better\n",
    "    save_total_limit=3,                # Reduced checkpoint limit (was 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe44ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import early stopping callback\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Initialize early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,      # Stop after 5 evaluations without improvement\n",
    "    early_stopping_threshold=0.001   # Minimum improvement required\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f2cefc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f5e2c608d24c4abaec258cd07b45f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942468961b1b4f569cbdf4bf8af82ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f8c345c7fb4c35b230a750b963edd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180367df7ab646508d921aca6cb602a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e718b52603ee413a97ff48e9e4b770fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c2ec0127af4ac9882ae528147d5e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861dd6715b6e4323af6af7f7c2cb584a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing eval dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize trainer with PEFT configuration\n",
    "# SFTTrainer will automatically handle the conversational format and apply chat templates\n",
    "trainer = SFTTrainer(\n",
    "    model=MODEL_NAME,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    processing_class=tokenizer,  # Pass tokenizer as processing_class\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    callbacks=[early_stopping],  # Use our configured early stopping callback\n",
    "    #completion_only=True,  # Only calculate loss on the assistant's response, not the full conversation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5c8dca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.12 GB\n",
      "GPU memory reserved : 0.22 GB\n",
      "PEFT config: LoRA with rank=32, alpha=64\n",
      "Target modules: {'down_proj', 'k_proj', 'gate_proj', 'o_proj', 'v_proj', 'q_proj', 'up_proj'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(f\"GPU memory reserved : {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "print(f\"PEFT config: LoRA with rank={peft_config.r}, alpha={peft_config.lora_alpha}\")\n",
    "print(f\"Target modules: {peft_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41e46aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/600 12:36 < 07:36, 0.49 it/s, Epoch 62/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.129100</td>\n",
       "      <td>0.127001</td>\n",
       "      <td>2.591401</td>\n",
       "      <td>87979.000000</td>\n",
       "      <td>0.394315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.113858</td>\n",
       "      <td>2.107718</td>\n",
       "      <td>176387.000000</td>\n",
       "      <td>0.419135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>0.081411</td>\n",
       "      <td>1.369299</td>\n",
       "      <td>264428.000000</td>\n",
       "      <td>0.445556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.067269</td>\n",
       "      <td>1.106834</td>\n",
       "      <td>352689.000000</td>\n",
       "      <td>0.466373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.059532</td>\n",
       "      <td>0.962033</td>\n",
       "      <td>440741.000000</td>\n",
       "      <td>0.481986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.056064</td>\n",
       "      <td>0.910817</td>\n",
       "      <td>526925.000000</td>\n",
       "      <td>0.488791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.052892</td>\n",
       "      <td>0.863394</td>\n",
       "      <td>615106.000000</td>\n",
       "      <td>0.497198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.050877</td>\n",
       "      <td>0.825032</td>\n",
       "      <td>703026.000000</td>\n",
       "      <td>0.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.048552</td>\n",
       "      <td>0.788505</td>\n",
       "      <td>791237.000000</td>\n",
       "      <td>0.500801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.046988</td>\n",
       "      <td>0.764803</td>\n",
       "      <td>879599.000000</td>\n",
       "      <td>0.500801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.046152</td>\n",
       "      <td>0.750437</td>\n",
       "      <td>967601.000000</td>\n",
       "      <td>0.503603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.045635</td>\n",
       "      <td>0.740454</td>\n",
       "      <td>1053850.000000</td>\n",
       "      <td>0.506405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.045564</td>\n",
       "      <td>0.736933</td>\n",
       "      <td>1141920.000000</td>\n",
       "      <td>0.507606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.044886</td>\n",
       "      <td>0.728947</td>\n",
       "      <td>1229938.000000</td>\n",
       "      <td>0.507606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.044450</td>\n",
       "      <td>0.726393</td>\n",
       "      <td>1318148.000000</td>\n",
       "      <td>0.507606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.0547679342230161, metrics={'train_runtime': 763.3183, 'train_samples_per_second': 2.882, 'train_steps_per_second': 0.786, 'total_flos': 917212849233408.0, 'train_loss': 0.0547679342230161, 'epoch': 62.54545454545455})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc6de138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "============================================================\n",
      "QUESTION:\n",
      "What are the common symptoms of diabetes?\n",
      "\n",
      "============================================================\n",
      "AI ASSISTANT RESPONSE:\n",
      "Diabetes is often symptomless, but it can cause:\n",
      "\n",
      "1. **Weight loss**: Difficulty eating or gaining weight.\n",
      "2. ** Hunger**: Irritability, malnutrition, or undereating.\n",
      "3 ** pedal numbness** (tingling in the hands and feet).\n",
      "4 **blindness** ( cataracts, glaucoma, or Kane's disease).\n",
      "5 **hearing loss** ( deafness, vertigo, or tinnitus).\n",
      "6 **vision loss**( cataracts, macular degeneration).\n",
      "7 **diabetic retinopathy** ( retinal damage).\n",
      "8 **nerve neuropathy** ( nerve damage).Course: Understanding Financial Markets and Institutions\n",
      "\n",
      "Welcome to this course on financial markets and institutions! You've likely heard about banks, stock exchanges\n",
      "============================================================\n",
      "\n",
      "============================== DEBUG ==============================\n",
      "FORMATTED INPUT:\n",
      "\"<|im_start|>system\\nYou are a careful medical assistant providing evidence-based information. Always end with 'This response was generated by AI. Please check with medical practitioners.'<|im_end|>\\n<|im_start|>user\\nWhat are the common symptoms of diabetes?<|im_end|>\\n<|im_start|>assistant\\n\"\n",
      "\n",
      "Input token length: 51\n",
      "Total output tokens: 201\n",
      "Generated tokens: 150\n"
     ]
    }
   ],
   "source": [
    "# Inference using TRL - directly from the trainer\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "# Use the trained model directly from trainer\n",
    "model = trainer.model\n",
    "tokenizer = trainer.processing_class\n",
    "\n",
    "# Example question for inference\n",
    "question = \"What are the common symptoms of diabetes?\"\n",
    "\n",
    "# Format the input using the same system prompt as a conversational format\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": cfg.system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "# Apply chat template (SFTTrainer should have set this up automatically)\n",
    "formatted_input = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(\n",
    "    formatted_input, \n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# Better generation configuration to prevent repetition\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    no_repeat_ngram_size=3,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(\"Generating response...\")\n",
    "\n",
    "# Generate response using the LoRA-tuned model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "# Better approach: Extract only the newly generated tokens\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs[0][input_length:]\n",
    "generated_response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"QUESTION:\")\n",
    "print(question)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AI ASSISTANT RESPONSE:\")\n",
    "print(generated_response if generated_response else \"[NO RESPONSE GENERATED]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Debug: Show the formatted input for comparison\n",
    "print(\"\\n\" + \"=\" * 30 + \" DEBUG \" + \"=\" * 30)\n",
    "print(\"FORMATTED INPUT:\")\n",
    "print(repr(formatted_input))\n",
    "print(f\"\\nInput token length: {input_length}\")\n",
    "print(f\"Total output tokens: {outputs[0].shape[0]}\")\n",
    "print(f\"Generated tokens: {len(generated_tokens)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
