# Simplified single configuration file
# All essential settings in one place

# Mode: train or inference
mode: train

# Global settings
seed: 816
output_dir: ./checkpoints/model

# Model configuration
model:
# models = {"microsoft/Phi-4-mini-instruct": "https://huggingface.co/microsoft/Phi-4-mini-instruct",
#           "HuggingFaceTB/SmolLM-135M-Instruct": "https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct",
#           "HuggingFaceTB/SmolLM3-3B": "https://huggingface.co/HuggingFaceTB/SmolLM3-3B"
# }
  name: HuggingFaceTB/SmolLM-135M-Instruct
  max_length: 512

# LoRA settings
lora:
  r: 16
  alpha: 32
  dropout: 0.1
  target_modules: [q_proj, v_proj, k_proj, o_proj]

# Data settings
data:
  train_file: ./data/my_custom_data.jsonl
  test_split: 0.1
  validation_split: 0.1
  system_prompt: >
    You are a careful medical assistant providing evidence-based information. 
    Your primary duty is to answer questions truthfully and do not recommend things likely to harm people.
    If you don't know the answer, simply response 'I am not sure of the answer, Dave.'
    Always end with 'This response was generated by AI. It is for educational and entertainment purposes only. Please check with medical practitioners.'

# Training settings
training:
  batch_size: 4
  learning_rate: 2e-4
  num_epochs: 3
  gradient_accumulation_steps: 8
  logging_steps: 10
  early_stopping_patience: 3

# Inference settings  
inference:
  adapter_path: ${output_dir}/my_custom_llm_${model_name}
  interactive: true
  demo_question: "What are the latest treatment guidelines for hypertension?"
  max_new_tokens: 512
  temperature: 0.6
  top_p: 0.9

# Quick presets (uncomment to use)
# For fast testing:
# training.num_epochs: 1
# training.batch_size: 1

# For production:  
# training.num_epochs: 10
# training.learning_rate: 1e-4