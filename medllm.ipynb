{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eac9367",
   "metadata": {},
   "source": [
    "# Medical Chatbot - LoRA Fine-tuning with Phi-4-mini-instruct\n",
    "\n",
    "This notebook demonstrates how to fine-tune a language model for medical assistance using **LoRA (Low-Rank Adaptation)** with 4-bit quantization (QLoRA) on **Phi-4-mini-instruct**.\n",
    "\n",
    "**Key Features:**\n",
    "- üíæ Memory-efficient training with 4-bit quantization\n",
    "- üéØ Parameter-efficient fine-tuning using LoRA\n",
    "- üìä Perplexity metrics using HuggingFace Evaluate library\n",
    "- üß™ Test set evaluation during training with custom callbacks\n",
    "- üí¨ Chat template formatting for conversational AI\n",
    "- üîß **Function calling support** for tools like internet search\n",
    "\n",
    "**Training Pipeline:**\n",
    "1. Load and prepare dataset\n",
    "2. Configure model with QLoRA\n",
    "3. Apply chat templates\n",
    "4. Train with metrics monitoring\n",
    "5. Evaluate and save adapters\n",
    "6. Test inference with function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ea5a2",
   "metadata": {},
   "source": [
    "## üîê Model Access\n",
    "\n",
    "**Phi-4-mini-instruct** is an ungated model, so no authentication required.\n",
    "\n",
    "**Why Phi-4?**\n",
    "- Improved reasoning and instruction-following\n",
    "- Native function calling support for tools\n",
    "- 128K token context length\n",
    "- Better multilingual support\n",
    "\n",
    "**Alternative Models:**\n",
    "- `meta-llama/Llama-3.2-3B-Instruct` (requires access)\n",
    "- `teknium/OpenHermes-2.5-Mistral-7B`\n",
    "- `google/gemma-2b-it`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4fc88",
   "metadata": {},
   "source": [
    "## üì¶ Dependencies\n",
    "\n",
    "Required libraries for LoRA fine-tuning with 4-bit quantization.\n",
    "\n",
    "NOTE: If you are using [pixi](https://pixi.sh) then the virtual environment will already be setup for you. \n",
    "\n",
    "```bash\n",
    "pixi install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (uncomment for a fresh environment)\n",
    "# %pip install -q \"transformers>=4.40.0\" \"datasets>=2.18.0\" \"peft>=0.11.0\" \"accelerate>=0.28.0\" \"bitsandbytes\" \"evaluate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e9772",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration & Imports\n",
    "\n",
    "Set up the training environment:\n",
    "- **Seed**: 816 for reproducibility\n",
    "- **Model**: microsoft/Phi-4-mini-instruct (3.8B parameters)\n",
    "- **Quantization**: 4-bit NF4 with double quantization\n",
    "- **Training**: 100 steps with batch size 1, gradient accumulation 8\n",
    "- **Learning Rate**: 2e-4 with 3% warmup\n",
    "- **Function Calling**: Support for tools (search, compute, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff124ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 816\n",
    "set_seed(SEED)\n",
    "\n",
    "# Configurable training parameters\n",
    "base_model = \"microsoft/Phi-4-mini-instruct\"\n",
    "train_file = \"./data/my_custom_data.jsonl\"\n",
    "output_dir = \"./checkpoints/llama3-lora-med\"\n",
    "max_length = 512  # Reduced from 2048 for 8GB GPU\n",
    "batch_size = 1\n",
    "lr = 2e-4\n",
    "max_steps = 10\n",
    "validate_steps = 5  # Evaluate test set every N steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d51639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define available tools for function calling\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"search_internet\",\n",
    "        \"description\": \"Search the internet for current medical information, research, and clinical guidelines\",\n",
    "        \"parameters\": {\n",
    "            \"query\": {\n",
    "                \"description\": \"The medical search query\",\n",
    "                \"type\": \"str\"\n",
    "            },\n",
    "            \"num_results\": {\n",
    "                \"description\": \"Number of search results to return\",\n",
    "                \"type\": \"int\",\n",
    "                \"default\": 5\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"retrieve_clinical_guidelines\",\n",
    "        \"description\": \"Retrieve clinical guidelines and best practices for specific conditions\",\n",
    "        \"parameters\": {\n",
    "            \"condition\": {\n",
    "                \"description\": \"The medical condition to retrieve guidelines for\",\n",
    "                \"type\": \"str\"\n",
    "            },\n",
    "            \"guideline_source\": {\n",
    "                \"description\": \"Source of guidelines (e.g., 'NICE', 'AHA', 'CDC', 'WHO')\",\n",
    "                \"type\": \"str\",\n",
    "                \"default\": \"general\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# System prompt for medical assistant with tool awareness\n",
    "system_prompt = (\n",
    "    \"You are a careful medical assistant with access to tools for finding current information. \"\n",
    "    \"You can search the internet for recent medical research and clinical guidelines. \"\n",
    "    \"Reason step by step, cite sources when available, and avoid guessing beyond provided information. \"\n",
    "    \"Your primary function is to not harm. If you are unsure, tell the user you don't know or will search for information. \"\n",
    "    \"Use tools proactively when questions require current information or specific guidelines. \"\n",
    "    \"Always end the conversation with 'This response was generated by AI. \"\n",
    "    \"Please check with professional medical practitioners to confirm the results are safe and appropriate.'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c563ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6dfb7",
   "metadata": {},
   "source": [
    "## üîß Function Calling & Tools\n",
    "\n",
    "**Phi-4 supports function calling** for accessing external tools. The system prompt includes tool definitions in JSON format.\n",
    "\n",
    "**Tool Format:**\n",
    "- Tools are wrapped in `<|tool|>` and `<|/tool|>` tokens\n",
    "- Model generates function calls when appropriate\n",
    "- Supports internet search, guideline retrieval, and more\n",
    "\n",
    "**Model Output Example:**\n",
    "```\n",
    "I'll search for the latest information about this condition.\n",
    "\n",
    "<|function_call|>\n",
    "search_internet\n",
    "{\"query\": \"recent research on hypertension treatment 2025\"}\n",
    "<|/function_call|>\n",
    "```\n",
    "\n",
    "During inference, you can capture these calls and execute actual tool logic, then feed results back to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a8658",
   "metadata": {},
   "source": [
    "## üìö Load Dataset\n",
    "\n",
    "Load the JSONL dataset and split into train/validation/test sets.\n",
    "\n",
    "**Dataset Format:**\n",
    "```json\n",
    "{\"instruction\": \"What is hypertension?\", \"response\": \"Hypertension, or high blood pressure, means ...\"}\n",
    "```\n",
    "\n",
    "**Splits:**\n",
    "- Training: 80% of data\n",
    "- Validation: 10% of data\n",
    "- Test: 10% of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f8d44",
   "metadata": {},
   "source": [
    "### Dataset Split Details\n",
    "\n",
    "The data is loaded from a JSONL file where each line contains a medical Q&A pair:\n",
    "- **instruction**: The user's question\n",
    "- **response**: The model's expected answer\n",
    "\n",
    "**Cascading Split Strategy:**\n",
    "- First, we split 80/20 to create training and temporary data\n",
    "- Then we split the temporary data 50/50 to create validation and test sets\n",
    "- This ensures three non-overlapping subsets for training, validation, and testing\n",
    "- Using the same seed (816) ensures reproducibility\n",
    "\n",
    "This approach prevents data leakage‚Äîthe model never sees test data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71597ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from JSONL file\n",
    "full_dataset = load_dataset(\"json\", data_files={\"train\": train_file})[\"train\"]\n",
    "\n",
    "# Split into train/validation/test using cascading train_test_split\n",
    "# First split: 80% train, 20% temp (for validation + test)\n",
    "split_1 = full_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "train_data = split_1[\"train\"]\n",
    "temp_data = split_1[\"test\"]\n",
    "\n",
    "# Second split: split the temp data 50/50 into validation and test\n",
    "split_2 = temp_data.train_test_split(test_size=0.5, seed=SEED)\n",
    "validation_data = split_2[\"train\"]\n",
    "test_data = split_2[\"test\"]\n",
    "\n",
    "# Create DatasetDict with all three splits\n",
    "raw_dataset = DatasetDict({\n",
    "    \"train\": train_data,\n",
    "    \"validation\": validation_data,\n",
    "    \"test\": test_data\n",
    "})\n",
    "\n",
    "print(raw_dataset)\n",
    "print(\"\\nExample from training set:\")\n",
    "print(raw_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db752b0",
   "metadata": {},
   "source": [
    "## ü§ñ Model Setup\n",
    "\n",
    "**Steps:**\n",
    "1. Load tokenizer and configure padding\n",
    "2. Load model in 4-bit quantization\n",
    "3. Prepare model for k-bit training\n",
    "4. Enable gradient checkpointing (saves memory)\n",
    "5. Attach LoRA adapters\n",
    "\n",
    "**LoRA Configuration:**\n",
    "- Rank (r): 16\n",
    "- Alpha: 32\n",
    "- Target modules: All attention and MLP layers\n",
    "- Dropout: 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99171f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Prepare for LoRA fine-tuning in 4-bit\n",
    "model.config.use_cache = False  # Required for gradient checkpointing\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Attach LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07ee92",
   "metadata": {},
   "source": [
    "### Understanding 4-bit Quantization\n",
    "\n",
    "**What is quantization?**\n",
    "\n",
    "Quantization reduces memory usage by representing model weights with fewer bits. Standard 16-bit (float16) weights become 4-bit weights, reducing memory by ~75%.\n",
    "\n",
    "**NF4 (Normal Float 4):**\n",
    "\n",
    "- Uses 4 bits instead of 16\n",
    "- Optimized for normal distributions of neural network weights\n",
    "- Double quantization: quantizes the quantization scale itself for more compression\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "- Phi-4-mini has 3.8B parameters. In float16, this needs ~8GB of VRAM\n",
    "- With 4-bit quantization, it fits in ~2GB\n",
    "- Enables fine-tuning on consumer GPUs (like NVIDIA RTX 3060/4090)\n",
    "\n",
    "**Trade-off:** \n",
    "\n",
    "Slight reduction in model precision, but with LoRA fine-tuning, the impact is minimal and results are comparable to full precision training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185c33f",
   "metadata": {},
   "source": [
    "## üî§ Data Preprocessing\n",
    "\n",
    "**Process:**\n",
    "1. **Format Examples**: Apply chat template (system, user, assistant roles)\n",
    "2. **Tokenize**: Convert text to token IDs with padding/truncation\n",
    "3. **Create Labels**: Clone input_ids for next-token prediction\n",
    "\n",
    "**Chat Template Example:**\n",
    "```\n",
    "<|system|>You are a careful medical assistant...<|end|>\n",
    "<|user|>What is hypertension?<|end|>\n",
    "<|assistant|>Hypertension is...<|end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5185c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"Apply chat template to each example.\"\"\"\n",
    "    messages: List[Dict[str, str]] = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": example.get(\"instruction\", \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": example.get(\"response\", \"\")},\n",
    "    ]\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    return example\n",
    "\n",
    "# Apply chat template to all examples\n",
    "formatted = raw_dataset.map(format_example, remove_columns=raw_dataset[\"train\"].column_names)\n",
    "\n",
    "\n",
    "def tokenize_batch(batch: Dict[str, List[str]]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Tokenize a batch of examples.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize all examples\n",
    "tokenized = formatted.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset = tokenized[\"train\"].with_format(\"torch\")\n",
    "eval_dataset = tokenized[\"validation\"].with_format(\"torch\")\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(eval_dataset)}\")\n",
    "print(\"\\nFirst 120 tokens of training example:\")\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"][:120]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12e4db6",
   "metadata": {},
   "source": [
    "### Chat Template & Tokenization Process\n",
    "\n",
    "**Step 1: Format Examples**\n",
    "The `format_example()` function applies Phi-4's chat template, which structures conversations with role markers:\n",
    "- `<|system|>`: System instructions and context\n",
    "- `<|user|>`: User's question\n",
    "- `<|assistant|>`: Model's response\n",
    "\n",
    "**Step 2: Tokenize**\n",
    "The `tokenize_batch()` function converts text to token IDs:\n",
    "- **max_length=512**: Truncate longer sequences (prevents GPU memory overflow)\n",
    "- **padding=\"max_length\"**: Pad shorter sequences to 512 tokens\n",
    "- **labels**: Clone of input_ids for supervised learning (predict next token)\n",
    "\n",
    "**Why labels matter:**\n",
    "During training, the model learns to predict each token's next token. The labels tell the trainer what the \"correct\" next token should be for each position. By using `input_ids` as labels, we're doing standard causal language modeling (predict the next word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0980bad",
   "metadata": {},
   "source": [
    "## üìä Metrics\n",
    "\n",
    "- **Perplexity**: exp(loss) - measures how well the model predicts the next token\n",
    "  - Lower is better. \n",
    "  - Lowest possible score is 1. \n",
    "  - A score of 5, for example, means the model is effectively chosing from a list of 5 possiblities for each next word completion.\n",
    "- Computed using HuggingFace Evaluate library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78394942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute perplexity metric from model predictions.\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Extract logits if predictions is a tuple\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    # Shift for next-token prediction (standard for causal LM)\n",
    "    shift_logits = predictions[..., :-1, :]\n",
    "    shift_labels = labels[..., 1:]\n",
    "    \n",
    "    # Flatten for loss calculation\n",
    "    shift_logits = shift_logits.reshape(-1, shift_logits.shape[-1])\n",
    "    shift_labels = shift_labels.reshape(-1)\n",
    "    \n",
    "    # Calculate cross-entropy loss (ignore padding tokens with -100)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(\n",
    "        torch.tensor(shift_logits, dtype=torch.float32),\n",
    "        torch.tensor(shift_labels, dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    # Perplexity is exp(loss)\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    \n",
    "    return {\"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f2dbc",
   "metadata": {},
   "source": [
    "### How Perplexity is Calculated\n",
    "\n",
    "Perplexity measures how \"surprised\" the model is by the correct next token:\n",
    "\n",
    "**Formula:** \n",
    "$$\\text{Perplexity} = e^{\\text{loss}}$$\n",
    "\n",
    "**Process:**\n",
    "1. For each token position, compute cross-entropy loss (how wrong the prediction was)\n",
    "2. Average the loss across all tokens\n",
    "3. Take exponential of the average loss\n",
    "\n",
    "**Interpretation:**\n",
    "- **Perplexity = 1**: Model is 100% confident in its predictions (perfect)\n",
    "- **Perplexity = 5**: Model effectively chooses from ~5 equally likely options\n",
    "- **Perplexity = 100**: Model is very uncertain about next tokens\n",
    "\n",
    "**For chatbots:** A perplexity of 10 on a test set is acceptable. Lower is always better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7789cb2",
   "metadata": {},
   "source": [
    "### Custom Validation Callback\n",
    "\n",
    "The `ValidationEvalCallback` class provides real-time monitoring of model performance on the validation set during training.\n",
    "\n",
    "**Why it's needed:**\n",
    "- Standard HuggingFace trainers only show training loss\n",
    "- We need to monitor validation loss/perplexity to detect overfitting\n",
    "- Allows us to see if the model is generalizing to unseen data\n",
    "\n",
    "**How it works:**\n",
    "- Runs at the end of each training step\n",
    "- Every N steps (defined by `validate_steps`), it evaluates on the validation set\n",
    "- Prints validation metrics so you can monitor training progress\n",
    "- Works in conjunction with `EarlyStoppingCallback` to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6621c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to evaluate validation set during training\n",
    "class ValidationEvalCallback(TrainerCallback):\n",
    "    \"\"\"Evaluate validation set periodically during training.\"\"\"\n",
    "    \n",
    "    def __init__(self, eval_dataset):\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.trainer_obj = None\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at the end of each training step.\"\"\"\n",
    "        if state.global_step % validate_steps == 0 and state.global_step > 0 and self.trainer_obj is not None:\n",
    "            eval_results = self.trainer_obj.predict(self.eval_dataset)\n",
    "            eval_loss = eval_results.metrics.get('test_loss', 'N/A')\n",
    "            eval_perp = eval_results.metrics.get('test_perplexity', 'N/A')\n",
    "            print(f\"\\n[Step {state.global_step}] Validation Loss: {eval_loss:.4f}, \"\n",
    "                  f\"Validation Perplexity: {eval_perp:.4f} \"\n",
    "                  \"(Lower is better. Lowest possible score is 1.)\")\n",
    "            \n",
    "\n",
    "# Initialize callback with validation dataset\n",
    "val_callback = ValidationEvalCallback(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33767f",
   "metadata": {},
   "source": [
    "### Training Arguments Explained\n",
    "\n",
    "Key parameters that control the training process:\n",
    "\n",
    "**Batch Size & Accumulation:**\n",
    "- `per_device_train_batch_size=1`: Process 1 example per GPU step\n",
    "- `gradient_accumulation_steps=4`: Accumulate gradients over 4 steps before updating weights\n",
    "- *Effective batch size = 1 √ó 4 = 4*\n",
    "\n",
    "**Learning Schedule:**\n",
    "- `learning_rate=2e-4`: Step size for weight updates (smaller = more stable but slower)\n",
    "- `warmup_ratio=0.03`: Gradually increase LR for first 3% of training (prevents instability)\n",
    "- `max_steps=10`: Train for 10 steps total (for demo; increase for production)\n",
    "\n",
    "**Checkpointing:**\n",
    "- `save_steps=50`: Save model every 50 steps\n",
    "- `save_total_limit=2`: Keep only the 2 most recent checkpoints\n",
    "- `load_best_model_at_end=True`: Load the best checkpoint after training completes\n",
    "\n",
    "**Evaluation:**\n",
    "- `eval_strategy=\"steps\"`: Evaluate during training (not just at end)\n",
    "- `eval_steps=5`: Evaluate every 5 steps\n",
    "- `metric_for_best_model=\"eval_loss\"`: Track validation loss\n",
    "- `greater_is_better=False`: Lower loss is better\n",
    "\n",
    "**Precision:**\n",
    "- `bf16=True` (if CUDA) or `fp16=True`: Use lower precision for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825011dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,  # Reduced from 8 for 8GB GPU\n",
    "    learning_rate=lr,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    fp16=not torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=[\"none\"],\n",
    "    eval_strategy=\"steps\",  # Enable evaluation during training\n",
    "    eval_steps=validate_steps,  # Evaluate every N steps\n",
    "    load_best_model_at_end=True,  # Load the best checkpoint at the end\n",
    "    metric_for_best_model=\"eval_loss\",  # Track validation loss\n",
    "    greater_is_better=False,  # Lower loss is better\n",
    ")\n",
    "\n",
    "\n",
    "# Early stopping: stop if validation loss doesn't improve for N evaluations\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # Stop if no improvement for 3 evaluations\n",
    "    early_stopping_threshold=0.001,  # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[val_callback, early_stopping],  # Add early stopping callback\n",
    ")\n",
    "\n",
    "# Set trainer reference in callback (needed for validation evaluation)\n",
    "val_callback.trainer_obj = trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Starting training with early stopping (patience=3)...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aababdf4",
   "metadata": {},
   "source": [
    "## üíæ Save Model\n",
    "\n",
    "Save the trained LoRA adapters and optionally merge with base model.\n",
    "\n",
    "**Saved Artifacts:**\n",
    "- LoRA adapter weights (lightweight, ~few MB)\n",
    "- Tokenizer configuration\n",
    "\n",
    "**Loading the adapter:** Use `PeftModel.from_pretrained(base_model, adapter_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57382e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter weights\n",
    "adapter_dir = os.path.join(output_dir, \"lora_adapter\")\n",
    "model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "print(f\"‚úÖ Adapter saved to {adapter_dir}\")\n",
    "\n",
    "# Optionally merge and save full model\n",
    "merge = os.environ.get(\"MERGE_LORA\", \"0\") == \"1\"\n",
    "if merge:\n",
    "    merged_dir = os.path.join(output_dir, \"merged\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    merged_model.save_pretrained(merged_dir)\n",
    "    tokenizer.save_pretrained(merged_dir)\n",
    "    print(f\"‚úÖ Merged model saved to {merged_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805e666",
   "metadata": {},
   "source": [
    "## üß™ Test Set Evaluation\n",
    "\n",
    "Evaluate the final model performance on the held-out test set.\n",
    "\n",
    "**Metrics:**\n",
    "- **Test Loss**: Cross-entropy loss on test examples\n",
    "- **Test Perplexity**: exp(loss) \n",
    "  - Lower is better. \n",
    "  - Lowest possible score is 1. \n",
    "  - A score of 5, for example, means the model is effectively chosing from a list of 5 possiblities for each next word completion.\n",
    "\n",
    "A perplexity of ~10-20 is typical for well-trained medical chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the held-out test set (first time using it!)\n",
    "test_formatted = raw_dataset[\"test\"].map(format_example, remove_columns=raw_dataset[\"test\"].column_names)\n",
    "test_tokenized = test_formatted.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "test_dataset = test_tokenized.with_format(\"torch\")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = trainer.predict(test_dataset)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä FINAL TEST METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Loss: {test_results.metrics.get('test_loss', 'N/A'):.4f}\")\n",
    "print(f\"Test Perplexity: {test_results.metrics.get('test_perplexity', 'N/A'):.4f}\")\n",
    "print(\"\\nAll metrics:\")\n",
    "print(test_results.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0b6dd",
   "metadata": {},
   "source": [
    "## üí¨ Inference Demo with Function Calling\n",
    "\n",
    "Test the fine-tuned model with a medical query. Phi-4 can generate function calls for tools.\n",
    "\n",
    "**Process:**\n",
    "1. Load base model in fp16\n",
    "2. Attach trained LoRA adapters\n",
    "3. Format query with chat template and tool definitions\n",
    "4. Generate response (may include function calls)\n",
    "5. Parse and handle function calls if present\n",
    "\n",
    "**Try modifying the question to see if the model decides to use tools!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b234429",
   "metadata": {},
   "source": [
    "### Inference Setup Steps\n",
    "\n",
    "**Step 1: Load Base Model in float16**\n",
    "- Use float16 precision (not quantized) to maximize quality for inference\n",
    "- Phi-4 can run inference on a single GPU with 8GB VRAM\n",
    "\n",
    "**Step 2: Attach LoRA Adapters**\n",
    "- Load the trained LoRA weights we saved earlier\n",
    "- These are lightweight (~few MB) compared to the base model\n",
    "- They modify the attention and feed-forward layers learned during fine-tuning\n",
    "\n",
    "**Step 3: Format Prompt with Tools**\n",
    "- Include the system prompt and tool definitions\n",
    "- Use Phi-4's special tokens: `<|system|>`, `<|user|>`, `<|assistant|>`\n",
    "- The model can now reference tools in its response\n",
    "\n",
    "**Step 4: Generate Response**\n",
    "- Set `temperature=0.3` for consistent, focused medical advice\n",
    "- `do_sample=False` with low temperature = deterministic greedy decoding\n",
    "- Higher `max_new_tokens` allows longer responses\n",
    "- The model may generate `<|function_call|>` blocks when it wants to use a tool\n",
    "\n",
    "**Output:**\n",
    "The model's response may include function call blocks if it decides to search for information. In a real application, you'd parse these calls and execute actual tool logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6778373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model and attach LoRA adapters\n",
    "adapter_dir = os.path.join(output_dir, \"lora_adapter\")\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "inference_model = PeftModel.from_pretrained(inference_model, adapter_dir)\n",
    "inference_model.eval()\n",
    "\n",
    "# Format tools for Phi-4 function calling\n",
    "tools_json = json.dumps(tools, indent=2)\n",
    "\n",
    "# Prepare medical query with tools\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"What are the latest treatment guidelines for hypertension?\"},\n",
    "]\n",
    "\n",
    "# Build prompt with tools for Phi-4\n",
    "# Note: This format is Phi-4 specific with <|tool|> tags\n",
    "system_with_tools = f\"{system_prompt}\\n\\nAvailable tools:\\n{tools_json}\"\n",
    "formatted_prompt = f\"<|system|>{system_with_tools}<|end|><|user|>What are the latest treatment guidelines for hypertension?<|end|><|assistant|>\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = inference_model.get_input_embeddings().weight.device\n",
    "inputs_tensor = inference_model.tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids.to(inference_model.device)\n",
    "\n",
    "# Generate response\n",
    "print(\"ü§ñ Generating response with function calling enabled...\\n\")\n",
    "gen = inference_model.generate(\n",
    "    inputs_tensor,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.3,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# Decode response\n",
    "response = inference_model.tokenizer.decode(gen[0], skip_special_tokens=False)\n",
    "print(\"Full Response:\")\n",
    "print(\"-\" * 60)\n",
    "print(response)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check for function calls in response\n",
    "if \"<|function_call|>\" in response:\n",
    "    print(\"\\n‚úÖ Model generated function call(s)!\")\n",
    "    print(\"Function calls detected - you can parse and execute these in a real application\")\n",
    "else:\n",
    "    print(\"\\nüìù Model provided direct answer without tool usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece6790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
