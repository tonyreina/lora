{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eac9367",
   "metadata": {},
   "source": [
    "# Medical Chatbot - LoRA Fine-tuning with Phi-4-mini-instruct\n",
    "\n",
    "This notebook demonstrates how to fine-tune a language model for medical assistance using **LoRA (Low-Rank Adaptation)** with 4-bit quantization (QLoRA) on **Phi-4-mini-instruct**.\n",
    "\n",
    "**Key Features:**\n",
    "- üíæ Memory-efficient training with 4-bit quantization\n",
    "- üéØ Parameter-efficient fine-tuning using LoRA\n",
    "- üìä Perplexity metrics using HuggingFace Evaluate library\n",
    "- üß™ Test set evaluation during training with custom callbacks\n",
    "- üí¨ Chat template formatting for conversational AI\n",
    "- üîß **Function calling support** for tools like internet search\n",
    "\n",
    "**Training Pipeline:**\n",
    "1. Load and prepare dataset\n",
    "2. Configure model with QLoRA\n",
    "3. Apply chat templates\n",
    "4. Train with metrics monitoring\n",
    "5. Evaluate and save adapters\n",
    "6. Test inference with function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ea5a2",
   "metadata": {},
   "source": [
    "## üîê Model Access\n",
    "\n",
    "**Phi-4-mini-instruct** is an ungated model, so no authentication required.\n",
    "\n",
    "**Why Phi-4?**\n",
    "- Improved reasoning and instruction-following\n",
    "- Native function calling support for tools\n",
    "- 128K token context length\n",
    "- Better multilingual support\n",
    "\n",
    "**Alternative Models:**\n",
    "- `meta-llama/Llama-3.2-3B-Instruct` (requires access)\n",
    "- `teknium/OpenHermes-2.5-Mistral-7B`\n",
    "- `google/gemma-2b-it`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4fc88",
   "metadata": {},
   "source": [
    "## üì¶ Dependencies\n",
    "\n",
    "Required libraries for LoRA fine-tuning with 4-bit quantization.\n",
    "\n",
    "NOTE: If you are using [pixi](https://pixi.sh) then the virtual environment will already be setup for you. \n",
    "\n",
    "```bash\n",
    "pixi install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (uncomment for a fresh environment)\n",
    "# %pip install -q \"transformers>=4.40.0\" \"datasets>=2.18.0\" \"peft>=0.11.0\" \"accelerate>=0.28.0\" \"bitsandbytes\" \"evaluate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e9772",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration & Imports\n",
    "\n",
    "Set up the training environment:\n",
    "- **Seed**: 816 for reproducibility\n",
    "- **Model**: microsoft/Phi-4-mini-instruct (3.8B parameters)\n",
    "- **Quantization**: 4-bit NF4 with double quantization\n",
    "- **Training**: 100 steps with batch size 1, gradient accumulation 8\n",
    "- **Learning Rate**: 2e-4 with 3% warmup\n",
    "- **Function Calling**: Support for tools (search, compute, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff124ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 816\n",
    "set_seed(SEED)\n",
    "\n",
    "# Configurable training parameters\n",
    "base_model = \"microsoft/Phi-4-mini-instruct\"\n",
    "train_file = \"./data/my_custom_data.jsonl\"\n",
    "output_dir = \"./checkpoints/llama3-lora-med\"\n",
    "max_length = 2048\n",
    "batch_size = 1\n",
    "lr = 2e-4\n",
    "max_steps = 100\n",
    "validate_steps = 50  # Evaluate test set every N steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d51639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define available tools for function calling\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"search_internet\",\n",
    "        \"description\": \"Search the internet for current medical information, research, and clinical guidelines\",\n",
    "        \"parameters\": {\n",
    "            \"query\": {\n",
    "                \"description\": \"The medical search query\",\n",
    "                \"type\": \"str\"\n",
    "            },\n",
    "            \"num_results\": {\n",
    "                \"description\": \"Number of search results to return\",\n",
    "                \"type\": \"int\",\n",
    "                \"default\": 5\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"retrieve_clinical_guidelines\",\n",
    "        \"description\": \"Retrieve clinical guidelines and best practices for specific conditions\",\n",
    "        \"parameters\": {\n",
    "            \"condition\": {\n",
    "                \"description\": \"The medical condition to retrieve guidelines for\",\n",
    "                \"type\": \"str\"\n",
    "            },\n",
    "            \"guideline_source\": {\n",
    "                \"description\": \"Source of guidelines (e.g., 'NICE', 'AHA', 'CDC', 'WHO')\",\n",
    "                \"type\": \"str\",\n",
    "                \"default\": \"general\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# System prompt for medical assistant with tool awareness\n",
    "system_prompt = (\n",
    "    \"You are a careful medical assistant with access to tools for finding current information. \"\n",
    "    \"You can search the internet for recent medical research and clinical guidelines. \"\n",
    "    \"Reason step by step, cite sources when available, and avoid guessing beyond provided information. \"\n",
    "    \"Your primary function is to not harm. If you are unsure, tell the user you don't know or will search for information. \"\n",
    "    \"Use tools proactively when questions require current information or specific guidelines. \"\n",
    "    \"Always end the conversation with 'This response was generated by AI. \"\n",
    "    \"Please check with professional medical practitioners to confirm the results are safe and appropriate.'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c563ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6dfb7",
   "metadata": {},
   "source": [
    "## üîß Function Calling & Tools\n",
    "\n",
    "**Phi-4 supports function calling** for accessing external tools. The system prompt includes tool definitions in JSON format.\n",
    "\n",
    "**Tool Format:**\n",
    "- Tools are wrapped in `<|tool|>` and `<|/tool|>` tokens\n",
    "- Model generates function calls when appropriate\n",
    "- Supports internet search, guideline retrieval, and more\n",
    "\n",
    "**Model Output Example:**\n",
    "```\n",
    "I'll search for the latest information about this condition.\n",
    "\n",
    "<|function_call|>\n",
    "search_internet\n",
    "{\"query\": \"recent research on hypertension treatment 2025\"}\n",
    "<|/function_call|>\n",
    "```\n",
    "\n",
    "During inference, you can capture these calls and execute actual tool logic, then feed results back to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a8658",
   "metadata": {},
   "source": [
    "## üìö Load Dataset\n",
    "\n",
    "Load the JSONL dataset and split into train/validation/test sets.\n",
    "\n",
    "**Dataset Format:**\n",
    "```json\n",
    "{\"instruction\": \"What is hypertension?\", \"response\": \"Hypertension, or high blood pressure, means ...\"}\n",
    "```\n",
    "\n",
    "**Splits:**\n",
    "- Training: 80% of data\n",
    "- Validation: 10% of data\n",
    "- Test: 10% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71597ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from JSONL file\n",
    "full_dataset = load_dataset(\"json\", data_files={\"train\": train_file})[\"train\"]\n",
    "\n",
    "# Split into train/validation/test using cascading train_test_split\n",
    "# First split: 80% train, 20% temp (for validation + test)\n",
    "split_1 = full_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "train_data = split_1[\"train\"]\n",
    "temp_data = split_1[\"test\"]\n",
    "\n",
    "# Second split: split the temp data 50/50 into validation and test\n",
    "split_2 = temp_data.train_test_split(test_size=0.5, seed=SEED)\n",
    "validation_data = split_2[\"train\"]\n",
    "test_data = split_2[\"test\"]\n",
    "\n",
    "# Create DatasetDict with all three splits\n",
    "raw_dataset = DatasetDict({\n",
    "    \"train\": train_data,\n",
    "    \"validation\": validation_data,\n",
    "    \"test\": test_data\n",
    "})\n",
    "\n",
    "print(raw_dataset)\n",
    "print(\"\\nExample from training set:\")\n",
    "print(raw_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db752b0",
   "metadata": {},
   "source": [
    "## ü§ñ Model Setup\n",
    "\n",
    "**Steps:**\n",
    "1. Load tokenizer and configure padding\n",
    "2. Load model in 4-bit quantization\n",
    "3. Prepare model for k-bit training\n",
    "4. Enable gradient checkpointing (saves memory)\n",
    "5. Attach LoRA adapters\n",
    "\n",
    "**LoRA Configuration:**\n",
    "- Rank (r): 16\n",
    "- Alpha: 32\n",
    "- Target modules: All attention and MLP layers\n",
    "- Dropout: 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99171f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Prepare for LoRA fine-tuning in 4-bit\n",
    "model.config.use_cache = False  # Required for gradient checkpointing\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Attach LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185c33f",
   "metadata": {},
   "source": [
    "## üî§ Data Preprocessing\n",
    "\n",
    "**Process:**\n",
    "1. **Format Examples**: Apply chat template (system, user, assistant roles)\n",
    "2. **Tokenize**: Convert text to token IDs with padding/truncation\n",
    "3. **Create Labels**: Clone input_ids for next-token prediction\n",
    "\n",
    "**Chat Template Example:**\n",
    "```\n",
    "<|system|>You are a careful medical assistant...<|end|>\n",
    "<|user|>What is hypertension?<|end|>\n",
    "<|assistant|>Hypertension is...<|end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5185c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"Apply chat template to each example.\"\"\"\n",
    "    messages: List[Dict[str, str]] = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": example.get(\"instruction\", \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": example.get(\"response\", \"\")},\n",
    "    ]\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    return example\n",
    "\n",
    "# Apply chat template to all examples\n",
    "formatted = raw_dataset.map(format_example, remove_columns=raw_dataset[\"train\"].column_names)\n",
    "\n",
    "\n",
    "def tokenize_batch(batch: Dict[str, List[str]]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Tokenize a batch of examples.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize all examples\n",
    "tokenized = formatted.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset = tokenized[\"train\"].with_format(\"torch\")\n",
    "eval_dataset = tokenized[\"validation\"].with_format(\"torch\")\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(eval_dataset)}\")\n",
    "print(\"\\nFirst 120 tokens of training example:\")\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"][:120]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0980bad",
   "metadata": {},
   "source": [
    "## üìä Metrics & Training Setup\n",
    "\n",
    "**Metrics:**\n",
    "- **Perplexity**: exp(loss) - measures how well the model predicts the next token\n",
    "  - Lower is better. \n",
    "  - Lowest possible score is 1. \n",
    "  - A score of 5, for example, means the model is effectively chosing from a list of 5 possiblities for each next word completion.\n",
    "- Computed using HuggingFace Evaluate library\n",
    "\n",
    "**Custom Callback:**\n",
    "- Evaluates **validation set** every 50 steps during training\n",
    "- Provides real-time feedback on model performance\n",
    "- **Test set is reserved for final evaluation only**\n",
    "\n",
    "**Training Arguments:**\n",
    "- Optimizer: paged_adamw_32bit (memory efficient)\n",
    "- Mixed precision: bf16 (if available) or fp16\n",
    "- Gradient accumulation: 8 steps (effective batch size = 8)\n",
    "- Checkpointing: Save every 50 steps, keep last 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute perplexity metric from model predictions.\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Extract logits if predictions is a tuple\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    # Shift for next-token prediction (standard for causal LM)\n",
    "    shift_logits = predictions[..., :-1, :]\n",
    "    shift_labels = labels[..., 1:]\n",
    "    \n",
    "    # Flatten for loss calculation\n",
    "    shift_logits = shift_logits.reshape(-1, shift_logits.shape[-1])\n",
    "    shift_labels = shift_labels.reshape(-1)\n",
    "    \n",
    "    # Calculate cross-entropy loss (ignore padding tokens with -100)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(\n",
    "        torch.tensor(shift_logits, dtype=torch.float32),\n",
    "        torch.tensor(shift_labels, dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    # Perplexity is exp(loss)\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    \n",
    "    return {\"perplexity\": perplexity}\n",
    "\n",
    "\n",
    "# No need to prepare test dataset here - it's already in raw_dataset[\"test\"]\n",
    "# We'll only use it for final evaluation at the end\n",
    "\n",
    "# Custom callback to evaluate validation set during training\n",
    "class ValidationEvalCallback(TrainerCallback):\n",
    "    \"\"\"Evaluate validation set periodically during training.\"\"\"\n",
    "    \n",
    "    def __init__(self, eval_dataset):\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.trainer_obj = None\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at the end of each training step.\"\"\"\n",
    "        if state.global_step % validate_steps == 0 and state.global_step > 0 and self.trainer_obj is not None:\n",
    "            eval_results = self.trainer_obj.predict(self.eval_dataset)\n",
    "            eval_loss = eval_results.metrics.get('test_loss', 'N/A')\n",
    "            eval_perp = eval_results.metrics.get('test_perplexity', 'N/A')\n",
    "            print(f\"\\n[Step {state.global_step}] Validation Loss: {eval_loss:.4f}, \"\n",
    "                  f\"Validation Perplexity: {eval_perp:.4f} \"\n",
    "                  \"(Lower is better. Lowest possible score is 1.)\")\n",
    "\n",
    "\n",
    "# Initialize training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=lr,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    fp16=not torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "# Initialize callback with validation dataset\n",
    "val_callback = ValidationEvalCallback(eval_dataset)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[val_callback],\n",
    ")\n",
    "\n",
    "# Set trainer reference in callback (needed for validation evaluation)\n",
    "val_callback.trainer_obj = trainer\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aababdf4",
   "metadata": {},
   "source": [
    "## üíæ Save Model\n",
    "\n",
    "Save the trained LoRA adapters and optionally merge with base model.\n",
    "\n",
    "**Saved Artifacts:**\n",
    "- LoRA adapter weights (lightweight, ~few MB)\n",
    "- Tokenizer configuration\n",
    "\n",
    "**Loading the adapter:** Use `PeftModel.from_pretrained(base_model, adapter_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57382e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter weights\n",
    "adapter_dir = os.path.join(output_dir, \"lora_adapter\")\n",
    "model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "print(f\"‚úÖ Adapter saved to {adapter_dir}\")\n",
    "\n",
    "# Optionally merge and save full model\n",
    "merge = os.environ.get(\"MERGE_LORA\", \"0\") == \"1\"\n",
    "if merge:\n",
    "    merged_dir = os.path.join(output_dir, \"merged\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    merged_model.save_pretrained(merged_dir)\n",
    "    tokenizer.save_pretrained(merged_dir)\n",
    "    print(f\"‚úÖ Merged model saved to {merged_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805e666",
   "metadata": {},
   "source": [
    "## üß™ Test Set Evaluation\n",
    "\n",
    "Evaluate the final model performance on the held-out test set.\n",
    "\n",
    "**Metrics:**\n",
    "- **Test Loss**: Cross-entropy loss on test examples\n",
    "- **Test Perplexity**: exp(loss) \n",
    "  - Lower is better. \n",
    "  - Lowest possible score is 1. \n",
    "  - A score of 5, for example, means the model is effectively chosing from a list of 5 possiblities for each next word completion.\n",
    "\n",
    "A perplexity of ~10-20 is typical for well-trained medical chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the held-out test set (first time using it!)\n",
    "test_formatted = raw_dataset[\"test\"].map(format_example, remove_columns=raw_dataset[\"test\"].column_names)\n",
    "test_tokenized = test_formatted.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "test_dataset = test_tokenized.with_format(\"torch\")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = trainer.predict(test_dataset)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä FINAL TEST METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Loss: {test_results.metrics.get('test_loss', 'N/A'):.4f}\")\n",
    "print(f\"Test Perplexity: {test_results.metrics.get('test_perplexity', 'N/A'):.4f}\")\n",
    "print(\"\\nAll metrics:\")\n",
    "print(test_results.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0b6dd",
   "metadata": {},
   "source": [
    "## üí¨ Inference Demo with Function Calling\n",
    "\n",
    "Test the fine-tuned model with a medical query. Phi-4 can generate function calls for tools.\n",
    "\n",
    "**Process:**\n",
    "1. Load base model in fp16\n",
    "2. Attach trained LoRA adapters\n",
    "3. Format query with chat template and tool definitions\n",
    "4. Generate response (may include function calls)\n",
    "5. Parse and handle function calls if present\n",
    "\n",
    "**Try modifying the question to see if the model decides to use tools!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6778373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model and attach LoRA adapters\n",
    "adapter_dir = os.path.join(output_dir, \"lora_adapter\")\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "inference_model = PeftModel.from_pretrained(inference_model, adapter_dir)\n",
    "inference_model.eval()\n",
    "\n",
    "# Format tools for Phi-4 function calling\n",
    "tools_json = json.dumps(tools, indent=2)\n",
    "\n",
    "# Prepare medical query with tools\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"What are the latest treatment guidelines for hypertension?\"},\n",
    "]\n",
    "\n",
    "# Build prompt with tools for Phi-4\n",
    "# Note: This format is Phi-4 specific with <|tool|> tags\n",
    "system_with_tools = f\"{system_prompt}\\n\\nAvailable tools:\\n{tools_json}\"\n",
    "formatted_prompt = f\"<|system|>{system_with_tools}<|end|><|user|>What are the latest treatment guidelines for hypertension?<|end|><|assistant|>\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = inference_model.get_input_embeddings().weight.device\n",
    "inputs_tensor = inference_model.tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids.to(inference_model.device)\n",
    "\n",
    "# Generate response\n",
    "print(\"ü§ñ Generating response with function calling enabled...\\n\")\n",
    "gen = inference_model.generate(\n",
    "    inputs_tensor,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.3,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# Decode response\n",
    "response = inference_model.tokenizer.decode(gen[0], skip_special_tokens=False)\n",
    "print(\"Full Response:\")\n",
    "print(\"-\" * 60)\n",
    "print(response)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check for function calls in response\n",
    "if \"<|function_call|>\" in response:\n",
    "    print(\"\\n‚úÖ Model generated function call(s)!\")\n",
    "    print(\"Function calls detected - you can parse and execute these in a real application\")\n",
    "else:\n",
    "    print(\"\\nüìù Model provided direct answer without tool usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece6790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
