{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eac9367",
   "metadata": {},
   "source": [
    "# Medical Chatbot - LoRA Fine-tuning with Phi-3.5\n",
    "\n",
    "This notebook demonstrates how to fine-tune a language model for medical assistance using **LoRA (Low-Rank Adaptation)** with 4-bit quantization (QLoRA). \n",
    "\n",
    "**Key Features:**\n",
    "- üíæ Memory-efficient training with 4-bit quantization\n",
    "- üéØ Parameter-efficient fine-tuning using LoRA\n",
    "- üìä Perplexity metrics using HuggingFace Evaluate library\n",
    "- üß™ Test set evaluation during training with custom callbacks\n",
    "- üí¨ Chat template formatting for conversational AI\n",
    "\n",
    "**Training Pipeline:**\n",
    "1. Load and prepare dataset\n",
    "2. Configure model with QLoRA\n",
    "3. Apply chat templates\n",
    "4. Train with metrics monitoring\n",
    "5. Evaluate and save adapters\n",
    "6. Test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ea5a2",
   "metadata": {},
   "source": [
    "## üîê Model Access\n",
    "\n",
    "**Important:** Some models may require authentication or have gated access.\n",
    "\n",
    "**Options:**\n",
    "- Use an ungated model like `microsoft/Phi-3.5-mini-instruct` (default)\n",
    "- For gated models (e.g., Llama), authenticate with: `pixi run huggingface-cli login`\n",
    "- Or set environment variable: `export HUGGINGFACE_TOKEN=your_token`\n",
    "\n",
    "**Alternative Models:**\n",
    "- `meta-llama/Llama-3.2-3B-Instruct` (requires access)\n",
    "- `teknium/OpenHermes-2.5-Mistral-7B`\n",
    "- `google/gemma-2b-it`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4fc88",
   "metadata": {},
   "source": [
    "## üì¶ Dependencies\n",
    "\n",
    "Required libraries for LoRA fine-tuning with 4-bit quantization.\n",
    "\n",
    "NOTE: If you are using [pixi](https://pixi.sh) then the virtual environment will already be setup for you. \n",
    "\n",
    "```bash\n",
    "pixi install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb6940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (uncomment for a fresh environment)\n",
    "# %pip install -q \"transformers>=4.40.0\" \"datasets>=2.18.0\" \"peft>=0.11.0\" \"accelerate>=0.28.0\" \"bitsandbytes\" \"evaluate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e9772",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration & Imports\n",
    "\n",
    "Set up the training environment:\n",
    "- **Seed**: 816 for reproducibility\n",
    "- **Model**: microsoft/Phi-3.5-mini-instruct (3.8B parameters)\n",
    "- **Quantization**: 4-bit NF4 with double quantization\n",
    "- **Training**: 100 steps with batch size 1, gradient accumulation 8\n",
    "- **Learning Rate**: 2e-4 with 3% warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcb2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff124ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 816\n",
    "set_seed(SEED)\n",
    "\n",
    "# Configurable training parameters\n",
    "base_model = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "train_file = \"./data/my_custom_data.jsonl\"\n",
    "output_dir = \"./checkpoints/llama3-lora-med\"\n",
    "max_length = 2048\n",
    "batch_size = 1\n",
    "lr = 2e-4\n",
    "max_steps = 100\n",
    "validate_steps = 50  # Evaluate test set every N steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d51639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for medical assistant\n",
    "system_prompt = (\n",
    "    \"You are a careful medical assistant. Reason step by step, cite sources when available, \"\n",
    "    \"and avoid guessing beyond the provided information. \"\n",
    "    \"Your primary function is to not harm. If you are unsure, then tell the user that you don't know the answer. \"\n",
    "    \"Always end the conversation with the words 'This response was generated by AI. \"\n",
    "    \"Please check with professional medical practictioners to confirm the results are safe and appropriate.'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c563ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a8658",
   "metadata": {},
   "source": [
    "## üìö Load Dataset\n",
    "\n",
    "Load the JSONL dataset and split into train/validation/test sets.\n",
    "\n",
    "**Dataset Format:**\n",
    "```json\n",
    "{\"instruction\": \"What is hypertension?\", \"response\": \"Hypertension, or high blood pressure, means ...\"}\n",
    "```\n",
    "\n",
    "**Splits:**\n",
    "- Training: 80% of data\n",
    "- Validation: 10% of data\n",
    "- Test: 10% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71597ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b6ada5d7f647dda9a252b26ca9b210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'response'],\n",
      "        num_rows: 80\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['instruction', 'response'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'response'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "})\n",
      "\n",
      "Example from training set:\n",
      "{'instruction': 'Describe the assessment and management of chronic kidney disease.', 'response': 'Stage by eGFR: Stage 1 (‚â•90), 2 (60-89), 3a (45-59), 3b (30-44), 4 (15-29), 5 (<15, ESRD). Slow progression: BP <120 mmHg (ACEi/ARB), glycemic control, avoid NSAIDs, limit protein (~0.6-0.8 g/kg), manage lipids, smoking cessation. Screen for complications: anemia (EPO if target Hgb 10-12), bone disease (check PTH, phosphate, calcium), cardiovascular disease. Nephrology referral if rapid decline, proteinuria, or stage 4-5. Prepare for renal replacement therapy (transplant, hemodialysis, peritoneal dialysis) when approaching ESRD.'}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from JSONL file\n",
    "full_dataset = load_dataset(\"json\", data_files={\"train\": train_file})[\"train\"]\n",
    "\n",
    "# Split into train/validation/test using cascading train_test_split\n",
    "# First split: 80% train, 20% temp (for validation + test)\n",
    "split_1 = full_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "train_data = split_1[\"train\"]\n",
    "temp_data = split_1[\"test\"]\n",
    "\n",
    "# Second split: split the temp data 50/50 into validation and test\n",
    "split_2 = temp_data.train_test_split(test_size=0.5, seed=SEED)\n",
    "validation_data = split_2[\"train\"]\n",
    "test_data = split_2[\"test\"]\n",
    "\n",
    "# Create DatasetDict with all three splits\n",
    "raw_dataset = DatasetDict({\n",
    "    \"train\": train_data,\n",
    "    \"validation\": validation_data,\n",
    "    \"test\": test_data\n",
    "})\n",
    "\n",
    "print(raw_dataset)\n",
    "print(\"\\nExample from training set:\")\n",
    "print(raw_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db752b0",
   "metadata": {},
   "source": [
    "## ü§ñ Model Setup\n",
    "\n",
    "**Steps:**\n",
    "1. Load tokenizer and configure padding\n",
    "2. Load model in 4-bit quantization\n",
    "3. Prepare model for k-bit training\n",
    "4. Enable gradient checkpointing (saves memory)\n",
    "5. Attach LoRA adapters\n",
    "\n",
    "**LoRA Configuration:**\n",
    "- Rank (r): 16\n",
    "- Alpha: 32\n",
    "- Target modules: All attention and MLP layers\n",
    "- Dropout: 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99171f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa56f81f44094e3586b68b1df5bc4b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,912,896 || all params: 3,829,992,448 || trainable%: 0.2327\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Prepare for LoRA fine-tuning in 4-bit\n",
    "model.config.use_cache = False  # Required for gradient checkpointing\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Attach LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185c33f",
   "metadata": {},
   "source": [
    "## üî§ Data Preprocessing\n",
    "\n",
    "**Process:**\n",
    "1. **Format Examples**: Apply chat template (system, user, assistant roles)\n",
    "2. **Tokenize**: Convert text to token IDs with padding/truncation\n",
    "3. **Create Labels**: Clone input_ids for next-token prediction\n",
    "\n",
    "**Chat Template Example:**\n",
    "```\n",
    "<|system|>You are a careful medical assistant...<|end|>\n",
    "<|user|>What is hypertension?<|end|>\n",
    "<|assistant|>Hypertension is...<|end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5185c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41aaf8f532bf447ba209c9c101630380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10da0d3710dd439c93e4bdcc9acab7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa77711b998d409f8cdeb7f91f5adc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8054acdc44d46119af15c1eeb701cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc73a1e847a4f6fa2f163e46d44c422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698bf878337c48059edf10036b5f5aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 80\n",
      "Validation examples: 10\n",
      "\n",
      "First 120 tokens of training example:\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def format_example(example: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"Apply chat template to each example.\"\"\"\n",
    "    messages: List[Dict[str, str]] = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": example.get(\"instruction\", \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": example.get(\"response\", \"\")},\n",
    "    ]\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    return example\n",
    "\n",
    "# Apply chat template to all examples\n",
    "formatted = raw_dataset.map(format_example, remove_columns=raw_dataset[\"train\"].column_names)\n",
    "\n",
    "\n",
    "def tokenize_batch(batch: Dict[str, List[str]]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Tokenize a batch of examples.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize all examples\n",
    "tokenized = formatted.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset = tokenized[\"train\"].with_format(\"torch\")\n",
    "eval_dataset = tokenized[\"validation\"].with_format(\"torch\")\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(eval_dataset)}\")\n",
    "print(\"\\nFirst 120 tokens of training example:\")\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"][:120]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0980bad",
   "metadata": {},
   "source": [
    "## üìä Metrics & Training Setup\n",
    "\n",
    "**Metrics:**\n",
    "- **Perplexity**: exp(loss) - measures how well the model predicts the next token\n",
    "  - Lower is better. \n",
    "  - Lowest possible score is 1. \n",
    "  - A score of 5, for example, means the model is effectively chosing from a list of 5 possiblities for each next word completion.\n",
    "- Computed using HuggingFace Evaluate library\n",
    "\n",
    "**Custom Callback:**\n",
    "- Evaluates **validation set** every 50 steps during training\n",
    "- Provides real-time feedback on model performance\n",
    "- **Test set is reserved for final evaluation only**\n",
    "\n",
    "**Training Arguments:**\n",
    "- Optimizer: paged_adamw_32bit (memory efficient)\n",
    "- Mixed precision: bf16 (if available) or fp16\n",
    "- Gradient accumulation: 8 steps (effective batch size = 8)\n",
    "- Checkpointing: Save every 50 steps, keep last 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473a060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 3:55:51, Epoch 19.90/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.921600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.713100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.701500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.630200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.557900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.527600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.364500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.287100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.093400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.066000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 50] Validation Loss: 0.8353, Validation Perplexity: 2.3047 (Lower is better)\n",
      "\n",
      "[Step 100] Validation Loss: 1.0489, Validation Perplexity: 2.8650 (Lower is better)\n",
      "\n",
      "[Step 150] Validation Loss: 1.5096, Validation Perplexity: 4.5627 (Lower is better)\n"
     ]
    }
   ],
   "source": [
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute perplexity metric from model predictions.\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Extract logits if predictions is a tuple\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    # Shift for next-token prediction (standard for causal LM)\n",
    "    shift_logits = predictions[..., :-1, :]\n",
    "    shift_labels = labels[..., 1:]\n",
    "    \n",
    "    # Flatten for loss calculation\n",
    "    shift_logits = shift_logits.reshape(-1, shift_logits.shape[-1])\n",
    "    shift_labels = shift_labels.reshape(-1)\n",
    "    \n",
    "    # Calculate cross-entropy loss (ignore padding tokens with -100)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(\n",
    "        torch.tensor(shift_logits, dtype=torch.float32),\n",
    "        torch.tensor(shift_labels, dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    # Perplexity is exp(loss)\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    \n",
    "    return {\"perplexity\": perplexity}\n",
    "\n",
    "\n",
    "# No need to prepare test dataset here - it's already in raw_dataset[\"test\"]\n",
    "# We'll only use it for final evaluation at the end\n",
    "\n",
    "# Custom callback to evaluate validation set during training\n",
    "class ValidationEvalCallback(TrainerCallback):\n",
    "    \"\"\"Evaluate validation set periodically during training.\"\"\"\n",
    "    \n",
    "    def __init__(self, eval_dataset):\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.trainer_obj = None\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at the end of each training step.\"\"\"\n",
    "        if state.global_step % validate_steps == 0 and state.global_step > 0 and self.trainer_obj is not None:\n",
    "            eval_results = self.trainer_obj.predict(self.eval_dataset)\n",
    "            eval_loss = eval_results.metrics.get('test_loss', 'N/A')\n",
    "            eval_perp = eval_results.metrics.get('test_perplexity', 'N/A')\n",
    "            print(f\"\\n[Step {state.global_step}] Validation Loss: {eval_loss:.4f}, \"\n",
    "                  f\"Validation Perplexity: {eval_perp:.4f} \"\n",
    "                  \"(Lower is better. Lowest possible score is 1.)\")\n",
    "\n",
    "\n",
    "# Initialize training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=lr,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    fp16=not torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "# Initialize callback with validation dataset\n",
    "val_callback = ValidationEvalCallback(eval_dataset)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[val_callback],\n",
    ")\n",
    "\n",
    "# Set trainer reference in callback (needed for validation evaluation)\n",
    "val_callback.trainer_obj = trainer\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aababdf4",
   "metadata": {},
   "source": [
    "## üíæ Save Model\n",
    "\n",
    "Save the trained LoRA adapters and optionally merge with base model.\n",
    "\n",
    "**Saved Artifacts:**\n",
    "- LoRA adapter weights (lightweight, ~few MB)\n",
    "- Tokenizer configuration\n",
    "\n",
    "**Loading the adapter:** Use `PeftModel.from_pretrained(base_model, adapter_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57382e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter weights\n",
    "adapter_dir = os.path.join(output_dir, \"lora_adapter\")\n",
    "model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "print(f\"‚úÖ Adapter saved to {adapter_dir}\")\n",
    "\n",
    "# Optionally merge and save full model\n",
    "merge = os.environ.get(\"MERGE_LORA\", \"0\") == \"1\"\n",
    "if merge:\n",
    "    merged_dir = os.path.join(output_dir, \"merged\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    merged_model.save_pretrained(merged_dir)\n",
    "    tokenizer.save_pretrained(merged_dir)\n",
    "    print(f\"‚úÖ Merged model saved to {merged_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805e666",
   "metadata": {},
   "source": [
    "## üß™ Test Set Evaluation\n",
    "\n",
    "Evaluate the final model performance on the held-out test set.\n",
    "\n",
    "**Metrics:**\n",
    "- **Test Loss**: Cross-entropy loss on test examples\n",
    "- **Test Perplexity**: exp(loss) \n",
    "  - Lower is better. \n",
    "  - Lowest possible score is 1. \n",
    "  - A score of 5, for example, means the model is effectively chosing from a list of 5 possiblities for each next word completion.\n",
    "\n",
    "A perplexity of ~10-20 is typical for well-trained medical chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the held-out test set (first time using it!)\n",
    "test_formatted = raw_dataset[\"test\"].map(format_example, remove_columns=raw_dataset[\"test\"].column_names)\n",
    "test_tokenized = test_formatted.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "test_dataset = test_tokenized.with_format(\"torch\")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = trainer.predict(test_dataset)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä FINAL TEST METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Loss: {test_results.metrics.get('test_loss', 'N/A'):.4f}\")\n",
    "print(f\"Test Perplexity: {test_results.metrics.get('test_perplexity', 'N/A'):.4f}\")\n",
    "print(\"\\nAll metrics:\")\n",
    "print(test_results.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0b6dd",
   "metadata": {},
   "source": [
    "## üí¨ Inference Demo\n",
    "\n",
    "Test the fine-tuned model with a medical query.\n",
    "\n",
    "**Process:**\n",
    "1. Load base model in fp16\n",
    "2. Attach trained LoRA adapters\n",
    "3. Format query with chat template\n",
    "4. Generate response with sampling disabled (deterministic)\n",
    "\n",
    "**Try modifying the question to test different medical scenarios!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6778373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model and attach LoRA adapters\n",
    "adapter_dir = os.path.join(output_dir, \"lora_adapter\")\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "inference_model = PeftModel.from_pretrained(inference_model, adapter_dir)\n",
    "inference_model.eval()\n",
    "\n",
    "# Prepare medical query\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"Give two differential considerations for chest pain.\"},\n",
    "]\n",
    "\n",
    "# Tokenize with chat template\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(inference_model.device)\n",
    "\n",
    "# Generate response\n",
    "print(\"ü§ñ Generating response...\\n\")\n",
    "gen = inference_model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.3,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# Decode and print response\n",
    "response = tokenizer.decode(gen[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "print(\"Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece6790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
