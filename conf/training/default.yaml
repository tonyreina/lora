# @package _global_
training:
  # Core training parameters
  batch_size: 4
  learning_rate: 2e-4
  num_epochs: 3    # Number of epochs to train
  gradient_accumulation_steps: 8
  
  # Simple logging frequency (steps)
  logging_steps: 10  # Log every 10 steps - reasonable for any dataset size
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 3
    metric: eval_loss
    
  # Optimizer settings
  optimizer:
    type: adamw
    weight_decay: 0.01
    
  # Scheduler settings
  scheduler:
    type: cosine
    warmup_ratio: 0.03