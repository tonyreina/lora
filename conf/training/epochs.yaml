# @package _global_
training:
  batch_size: 2
  learning_rate: 1e-4
  
  # Training duration - epoch-based configuration
  max_steps: null      # null means use epochs instead
  num_epochs: 5        # Number of epochs to train
  
  # For epoch-based training, these are ignored (evaluation/saving happens per epoch)
  validate_steps: null
  eval_steps: null
  save_steps: null
  
  gradient_accumulation_steps: 4
  warmup_steps: 10
  logging_steps: 5
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 2  # Number of epochs to wait for improvement
    metric: eval_loss
    
  # Optimizer settings
  optimizer:
    type: adamw
    weight_decay: 0.01
    
  # Scheduler settings
  scheduler:
    type: cosine
    warmup_ratio: 0.05