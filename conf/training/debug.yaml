# @package _global_
training:
  batch_size: 1
  learning_rate: 5e-4
  
  # Training duration - use either max_steps OR num_epochs
  # When both are specified, max_steps takes precedence
  max_steps: null  # null means use epochs instead
  num_epochs: 1    # Number of epochs to train (debug mode)
  
  validate_steps: 2
  gradient_accumulation_steps: 1
  warmup_steps: 1
  logging_steps: 1
  save_steps: 2
  eval_steps: 2
  
  # Early stopping
  early_stopping:
    enabled: false
    patience: 3
    metric: eval_loss
    
  # Optimizer settings
  optimizer:
    type: adamw
    weight_decay: 0.001
    
  # Scheduler settings
  scheduler:
    type: linear