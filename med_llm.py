# %% [markdown]
# # Medical Chatbot - LoRA Fine-tuning with Phi-4-mini-instruct
# 
# This notebook demonstrates how to fine-tune a language model for medical assistance using **LoRA (Low-Rank Adaptation)** with 4-bit quantization (QLoRA) on **Phi-4-mini-instruct**.
# 
# **Key Features:**
# - üíæ Memory-efficient training with 4-bit quantization
# - üéØ Parameter-efficient fine-tuning using LoRA
# - üìä Perplexity metrics using HuggingFace Evaluate library
# - üß™ Test set evaluation during training with custom callbacks
# - üí¨ Chat template formatting for conversational AI
# - üîß **Function calling support** for tools like internet search
# 
# **Training Pipeline:**
# 1. Load and prepare dataset
# 2. Configure model with QLoRA
# 3. Apply chat templates
# 4. Train with metrics monitoring
# 5. Evaluate and save adapters
# 6. Test inference with function calling

# %% [markdown]
# ## üîê Model Access
# 
# **Phi-4-mini-instruct** is an ungated model, so no authentication required.
# 
# **Why Phi-4?**
# - Improved reasoning and instruction-following
# - Native function calling support for tools
# - 128K token context length
# - Better multilingual support
# 
# **Alternative Models:**
# - `meta-llama/Llama-3.2-3B-Instruct` (requires access)
# - `teknium/OpenHermes-2.5-Mistral-7B`
# - `google/gemma-2b-it`

# %% [markdown]
# ## üì¶ Dependencies
# 
# Required libraries for LoRA fine-tuning with 4-bit quantization.
# 
# NOTE: If you are using [pixi](https://pixi.sh) then the virtual environment will already be setup for you. 
# 
# ```bash
# pixi install
# ```

# %%
# Install dependencies if needed (uncomment for a fresh environment)
# %pip install -q "transformers>=4.40.0" "datasets>=2.18.0" "peft>=0.11.0" "accelerate>=0.28.0" "bitsandbytes" "evaluate"

# %% [markdown]
# ## ‚öôÔ∏è Configuration & Imports
# 
# Set up the training environment:
# - **Seed**: 816 for reproducibility
# - **Model**: microsoft/Phi-4-mini-instruct (3.8B parameters)
# - **Quantization**: 4-bit NF4 with double quantization
# - **Training**: 100 steps with batch size 1, gradient accumulation 8
# - **Learning Rate**: 2e-4 with 3% warmup
# - **Function Calling**: Support for tools (search, compute, etc.)

# %%
import os
import json
from dataclasses import dataclass
from typing import Dict, List

import torch
import numpy as np
import evaluate
from datasets import load_dataset, DatasetDict
from huggingface_hub import login
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
    TrainerCallback,
    EarlyStoppingCallback,
    set_seed,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel

# %%
SEED = 816
set_seed(SEED)

# Configurable training parameters
base_model = "microsoft/Phi-4-mini-instruct"
train_file = "./data/my_custom_data.jsonl"
output_dir = "./checkpoints/llama3-lora-med"
max_length = 512  # Reduced from 2048 for 8GB GPU
batch_size = 1
lr = 2e-4
max_steps = 10
validate_steps = 5  # Evaluate test set every N steps

# %%
# Define available tools for function calling
tools = [
    {
        "name": "search_internet",
        "description": "Search the internet for current medical information, research, and clinical guidelines",
        "parameters": {
            "query": {
                "description": "The medical search query",
                "type": "str"
            },
            "num_results": {
                "description": "Number of search results to return",
                "type": "int",
                "default": 5
            }
        }
    },
    {
        "name": "retrieve_clinical_guidelines",
        "description": "Retrieve clinical guidelines and best practices for specific conditions.",
        "parameters": {
            "condition": {
                "description": "The medical condition to retrieve guidelines for",
                "type": "str"
            },
            "guideline_source": {
                "description": "Source of guidelines (e.g., 'NICE', 'AHA', 'CDC', 'WHO', 'JAMA', 'NEJM', 'American Cancer Society')",
                "type": "str",
                "default": "general"
            }
        }
    }
]

# System prompt for medical assistant with tool awareness
system_prompt = (
    "You are a careful medical assistant with access to tools for finding current information. "
    "You can search the internet for recent medical research and clinical guidelines. "
    "Reason step by step, cite sources when available, and avoid guessing beyond provided information. "
    "Your primary function is to not harm. If you are unsure, tell the user you don't know or will search for information. "
    "Use tools proactively when questions require current information or specific guidelines. "
    "Always end the conversation with 'This response was generated by AI. "
    "Please check with professional medical practitioners to confirm the results are safe and appropriate.'"
)

# %%
# 4-bit quantization configuration for memory efficiency
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# %% [markdown]
# ## üîß Function Calling & Tools
# 
# **Phi-4 supports function calling** for accessing external tools. The system prompt includes tool definitions in JSON format.
# 
# **Tool Format:**
# - Tools are wrapped in `<|tool|>` and `<|/tool|>` tokens
# - Model generates function calls when appropriate
# - Supports internet search, guideline retrieval, and more
# 
# **Model Output Example:**
# ```
# I'll search for the latest information about this condition.
# 
# <|function_call|>
# search_internet
# {"query": "recent research on hypertension treatment 2025"}
# <|/function_call|>
# ```
# 
# During inference, you can capture these calls and execute actual tool logic, then feed results back to the model.

# %% [markdown]
# ## üìö Load Dataset
# 
# Load the JSONL dataset and split into train/validation/test sets.
# 
# **Dataset Format:**
# ```json
# {"instruction": "What is hypertension?", "response": "Hypertension, or high blood pressure, means ..."}
# ```
# 
# **Splits:**
# - Training: 80% of data
# - Validation: 10% of data
# - Test: 10% of data

# %% [markdown]
# ### Dataset Split Details
# 
# The data is loaded from a JSONL file where each line contains a medical Q&A pair:
# - **instruction**: The user's question
# - **response**: The model's expected answer
# 
# **Cascading Split Strategy:**
# - First, we split 80/20 to create training and temporary data
# - Then we split the temporary data 50/50 to create validation and test sets
# - This ensures three non-overlapping subsets for training, validation, and testing
# - Using the same seed (816) ensures reproducibility
# 
# This approach prevents data leakage‚Äîthe model never sees test data during training.

# %%
# Load dataset from JSONL file
full_dataset = load_dataset("json", data_files={"train": train_file})["train"]

# Split into train/validation/test using cascading train_test_split
# First split: 80% train, 20% temp (for validation + test)
split_1 = full_dataset.train_test_split(test_size=0.2, seed=SEED)
train_data = split_1["train"]
temp_data = split_1["test"]

# Second split: split the temp data 50/50 into validation and test
split_2 = temp_data.train_test_split(test_size=0.5, seed=SEED)
validation_data = split_2["train"]
test_data = split_2["test"]

# Create DatasetDict with all three splits
raw_dataset = DatasetDict({
    "train": train_data,
    "validation": validation_data,
    "test": test_data
})

print(raw_dataset)
print("\nExample from training set:")
print(raw_dataset["train"][0])

# %% [markdown]
# ## ü§ñ Model Setup
# 
# **Steps:**
# 1. Load tokenizer and configure padding
# 2. Load model in 4-bit quantization
# 3. Prepare model for k-bit training
# 4. Enable gradient checkpointing (saves memory)
# 5. Attach LoRA adapters
# 
# **LoRA Configuration:**
# - Rank (r): 16
# - Alpha: 32
# - Target modules: All attention and MLP layers
# - Dropout: 5%

# %%
# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

# Load model with 4-bit quantization
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=bnb_config,
    device_map="auto",
)

# Prepare for LoRA fine-tuning in 4-bit
model.config.use_cache = False  # Required for gradient checkpointing
model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

# Attach LoRA adapters
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# %% [markdown]
# ### Understanding 4-bit Quantization
# 
# **What is quantization?**
# 
# Quantization reduces memory usage by representing model weights with fewer bits. Standard 16-bit (float16) weights become 4-bit weights, reducing memory by ~75%.
# 
# **NF4 (Normal Float 4):**
# 
# - Uses 4 bits instead of 16
# - Optimized for normal distributions of neural network weights
# - Double quantization: quantizes the quantization scale itself for more compression
# 
# **Why this matters:**
# 
# - Phi-4-mini has 3.8B parameters. In float16, this needs ~8GB of VRAM
# - With 4-bit quantization, it fits in ~2GB
# - Enables fine-tuning on consumer GPUs (like NVIDIA RTX 3060/4090)
# 
# **Trade-off:** 
# 
# Slight reduction in model precision, but with LoRA fine-tuning, the impact is minimal and results are comparable to full precision training.

# %% [markdown]
# ## üî§ Data Preprocessing
# 
# **Process:**
# 1. **Format Examples**: Apply chat template (system, user, assistant roles)
# 2. **Tokenize**: Convert text to token IDs with padding/truncation
# 3. **Create Labels**: Clone input_ids for next-token prediction
# 
# **Chat Template Example:**
# ```
# <|system|>You are a careful medical assistant...<|end|>
# <|user|>What is hypertension?<|end|>
# <|assistant|>Hypertension is...<|end|>
# ```

# %%
def format_example(example: Dict[str, str]) -> Dict[str, str]:
    """Apply chat template to each example."""
    messages: List[Dict[str, str]] = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": example.get("instruction", "")},
        {"role": "assistant", "content": example.get("response", "")},
    ]
    example["text"] = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False,
    )
    return example

# Apply chat template to all examples
formatted = raw_dataset.map(format_example, remove_columns=raw_dataset["train"].column_names)


def tokenize_batch(batch: Dict[str, List[str]]) -> Dict[str, torch.Tensor]:
    """Tokenize a batch of examples."""
    tokenized = tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
    )
    tokenized["labels"] = tokenized["input_ids"].clone()
    return tokenized

# Tokenize all examples
tokenized = formatted.map(tokenize_batch, batched=True, remove_columns=["text"])
train_dataset = tokenized["train"].with_format("torch")
eval_dataset = tokenized["validation"].with_format("torch")

print(f"Training examples  : {len(train_dataset)}")
print(f"Validation examples: {len(eval_dataset)}")
print("\nFirst 120 tokens of training example:")
print(tokenizer.decode(train_dataset[0]["input_ids"][:120]))

# %% [markdown]
# ### Chat Template & Tokenization Process
# 
# **Step 1: Format Examples**
# The `format_example()` function applies Phi-4's chat template, which structures conversations with role markers:
# - `<|system|>`: System instructions and context
# - `<|user|>`: User's question
# - `<|assistant|>`: Model's response
# 
# **Step 2: Tokenize**
# The `tokenize_batch()` function converts text to token IDs:
# - **max_length=512**: Truncate longer sequences (prevents GPU memory overflow)
# - **padding="max_length"**: Pad shorter sequences to 512 tokens
# - **labels**: Clone of input_ids for supervised learning (predict next token)
# 
# **Why labels matter:**
# During training, the model learns to predict each token's next token. The labels tell the trainer what the "correct" next token should be for each position. By using `input_ids` as labels, we're doing standard causal language modeling (predict the next word).

# %%
# Data collator for causal language modeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# %% [markdown]
# ### Training Arguments Explained
# 
# Key parameters that control the training process:
# 
# **Batch Size & Accumulation:**
# - `per_device_train_batch_size=1`: Process 1 example per GPU step
# - `gradient_accumulation_steps=4`: Accumulate gradients over 4 steps before updating weights
# - *Effective batch size = 1 √ó 4 = 4*
# 
# **Learning Schedule:**
# - `learning_rate=2e-4`: Step size for weight updates (smaller = more stable but slower)
# - `warmup_ratio=0.03`: Gradually increase LR for first 3% of training (prevents instability)
# - `max_steps=10`: Train for 10 steps total (for demo; increase for production)
# 
# **Checkpointing:**
# - `save_steps=50`: Save model every 50 steps
# - `save_total_limit=2`: Keep only the 2 most recent checkpoints
# - `load_best_model_at_end=True`: Load the best checkpoint after training completes
# 
# **Evaluation:**
# - `eval_strategy="steps"`: Evaluate during training (not just at end)
# - `eval_steps=5`: Evaluate every 5 steps
# - `metric_for_best_model="eval_loss"`: Track validation loss
# - `greater_is_better=False`: Lower loss is better
# 
# **Precision:**
# - `bf16=True` (if CUDA) or `fp16=True`: Use lower precision for memory efficiency

# %%
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=batch_size,
    gradient_accumulation_steps=4,  # Reduced from 8 for 8GB GPU
    learning_rate=lr,
    max_steps=max_steps,
    warmup_ratio=0.03,
    logging_steps=5,
    save_steps=50,
    save_total_limit=2,
    bf16=torch.cuda.is_available(),
    fp16=not torch.cuda.is_available(),
    gradient_checkpointing=True,
    optim="paged_adamw_32bit",
    report_to=["none"],
    eval_strategy="steps",  # Enable evaluation during training
    eval_steps=validate_steps,  # Evaluate every N steps
    load_best_model_at_end=True,  # Load the best checkpoint at the end
    metric_for_best_model="eval_loss",  # Track validation loss
    greater_is_better=False,  # Lower loss is better
)


# Early stopping: stop if validation loss doesn't improve for N evaluations
early_stopping = EarlyStoppingCallback(
    early_stopping_patience=3,  # Stop if no improvement for 3 evaluations
    early_stopping_threshold=0.001,  # Minimum improvement threshold
)

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
    callbacks=[early_stopping], 
)


# %%
print("\nüöÄ Starting training with early stopping (patience=3)...")
trainer.train()

print("\n\nFinished training.\n\n")
# %% [markdown]
# ## üíæ Save Model
# 
# Save the trained LoRA adapters and optionally merge with base model.
# 
# **Saved Artifacts:**
# - LoRA adapter weights (lightweight, ~few MB)
# - Tokenizer configuration
# 
# **Loading the adapter:** Use `PeftModel.from_pretrained(base_model, adapter_path)`

# %%
# Save LoRA adapter weights
adapter_dir = os.path.join(output_dir, "lora_adapter")
model.save_pretrained(adapter_dir)
tokenizer.save_pretrained(adapter_dir)
print(f"‚úÖ Adapter saved to {adapter_dir}")

# Optionally merge and save full model
merge = os.environ.get("MERGE_LORA", "0") == "1"
if merge:
    merged_dir = os.path.join(output_dir, "merged")
    merged_model = model.merge_and_unload()
    merged_model.save_pretrained(merged_dir)
    tokenizer.save_pretrained(merged_dir)
    print(f"‚úÖ Merged model saved to {merged_dir}")

# %% [markdown]
# ## üß™ Test Set Evaluation
# 
# Evaluate the final model performance on the held-out test set.
# 
# **Metrics:**
# - **Test Loss**: Cross-entropy loss on test examples
# - **Test Perplexity**: exp(loss) 
#   - Lower is better. 
#   - Lowest possible score is 1. 
#   - A score of 5, for example, means the model is effectively chosing from a list of 5 possiblities for each next word completion.
# 
# A perplexity of ~10 is acceptable for well-trained chatbots. Phi-4-mini-instruct has a perplexity of 8.48 on the [wikitext-2-raw-v1](https://rloganiv.github.io/linked-wikitext-2/#/) dataset

# %%
# Evaluate on the held-out test set (first time using it!)
test_formatted = raw_dataset["test"].map(format_example, remove_columns=raw_dataset["test"].column_names)
test_tokenized = test_formatted.map(tokenize_batch, batched=True, remove_columns=["text"])
test_dataset = test_tokenized.with_format("torch")

print(f"Test dataset size: {len(test_dataset)}")

# Evaluate on test set
test_results = trainer.predict(test_dataset)
print("\n" + "="*50)
print("üìä FINAL TEST METRICS")
print("="*50)
print(f"Test Loss: {test_results.metrics.get('test_loss', 'N/A'):.4f}")
print("\nAll metrics:")
print(test_results.metrics)

# %% [markdown]
# ### Memory Cleanup
# 
# Before loading the inference model, we need to free up GPU memory used by the training process. This prevents out-of-memory (OOM) errors when loading the larger inference model in float16.

# %% [markdown]
# ## üí¨ Inference Demo with Function Calling
# 
# Test the fine-tuned model with a medical query. Phi-4 can generate function calls for tools.
# 
# **Process:**
# 1. Load base model in fp16
# 2. Attach trained LoRA adapters
# 3. Format query with chat template and tool definitions
# 4. Generate response (may include function calls)
# 5. Parse and handle function calls if present
# 
# **Try modifying the question to see if the model decides to use tools!**

# %%
# Free up GPU memory from training
import gc
del trainer, model, train_dataset, eval_dataset
gc.collect()
torch.cuda.empty_cache()
print("‚úÖ Memory cleaned up for inference")

# %% [markdown]
# ### Inference Setup Steps
# 
# **Step 1: Load Base Model in float16**
# - Use float16 precision (not quantized) to maximize quality for inference
# - Phi-4 can run inference on a single GPU with 8GB VRAM
# 
# **Step 2: Attach LoRA Adapters**
# - Load the trained LoRA weights we saved earlier
# - These are lightweight (~few MB) compared to the base model
# - They modify the attention and feed-forward layers learned during fine-tuning
# 
# **Step 3: Format Prompt with Tools**
# - Include the system prompt and tool definitions
# - Use Phi-4's special tokens: `<|system|>`, `<|user|>`, `<|assistant|>`
# - The model can now reference tools in its response
# 
# **Step 4: Generate Response**
# - Set `temperature=0.3` for consistent, focused medical advice
# - `do_sample=False` with low temperature = deterministic greedy decoding
# - Higher `max_new_tokens` allows longer responses
# - The model may generate `<|function_call|>` blocks when it wants to use a tool
# 
# **Output:**
# The model's response may include function call blocks if it decides to search for information. In a real application, you'd parse these calls and execute actual tool logic.

# %%
# Load base model and attach LoRA adapters
adapter_dir = os.path.join(output_dir, "lora_adapter")

# Create offload directory for proper model dispatching
offload_dir = "./offload_tmp"
os.makedirs(offload_dir, exist_ok=True)

# Load with more conservative memory management
inference_model = AutoModelForCausalLM.from_pretrained(
    base_model,
    device_map="auto",
    dtype=torch.float16,
    low_cpu_mem_usage=True,
    offload_folder=offload_dir,
)
# Load adapter without additional offload_dir parameter to avoid conflicts
inference_model = PeftModel.from_pretrained(inference_model, adapter_dir)
inference_model.eval()

# Reload tokenizer (it was deleted in memory cleanup)
inference_tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)
if inference_tokenizer.pad_token is None:
    inference_tokenizer.pad_token = inference_tokenizer.unk_token or "<pad>"
    # Add pad token to tokenizer if it doesn't exist
    if inference_tokenizer.pad_token == "<pad>" and "<pad>" not in inference_tokenizer.vocab:
        inference_tokenizer.add_special_tokens({"pad_token": "<pad>"})
        inference_model.resize_token_embeddings(len(inference_tokenizer))

# Format tools for Phi-4 function calling
tools_json = json.dumps(tools, indent=2)

# Prepare medical query with tools
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "What are the latest treatment guidelines for hypertension?"},
]

# Build prompt with tools for Phi-4
# Note: This format is Phi-4 specific with <|tool|> tags
system_with_tools = f"{system_prompt}\n\nAvailable tools:\n{tools_json}"
formatted_prompt = f"<|system|>{system_with_tools}<|end|><|user|>What are the latest treatment guidelines for hypertension?<|end|><|assistant|>"

# Tokenize input
inputs = inference_tokenizer(
    formatted_prompt,
    return_tensors="pt",
    padding=True,
    truncation=True,
)
inputs_tensor = inputs.input_ids.to(inference_model.device)
attention_mask = inputs.attention_mask.to(inference_model.device)

# Generate response
print("ü§ñ Generating response with function calling enabled...\n")
gen = inference_model.generate(
    inputs_tensor,
    attention_mask=attention_mask,
    max_new_tokens=300,
    do_sample=True,
    temperature=0.3,
    pad_token_id=inference_tokenizer.pad_token_id,
)

# Decode response
response = inference_tokenizer.decode(gen[0], skip_special_tokens=False)
print("Full Response:")
print("-" * 60)
print(response)
print("-" * 60)

# Extract just the assistant's response (after the last <|assistant|> token)
if "<|assistant|>" in response:
    assistant_response = response.split("<|assistant|>")[-1]
    # Clean up any trailing tokens
    if "<|end|>" in assistant_response:
        assistant_response = assistant_response.split("<|end|>")[0]
    
    print("\nüéØ Model's Response to User:")
    print("=" * 60)
    print(assistant_response.strip())
    print("=" * 60)

# Check for function calls in response and execute them
if (("[{" in assistant_response and '"name"' in assistant_response) or 
    ("search_internet(" in assistant_response or "retrieve_clinical_guidelines(" in assistant_response)):
    print("\n‚úÖ Model generated function call(s)!")
    
    # Extract function calls (handle multiple formats)
    import re
    import json
    
    function_results = []
    source_urls = []
    
    # Try JSON format first
    json_pattern = r'\[{.*?}\]'
    json_matches = re.findall(json_pattern, assistant_response, re.DOTALL)
    
    # Also try function call format
    func_pattern = r'(\w+)\((.*?)\)'
    func_matches = re.findall(func_pattern, assistant_response)
    
    all_calls = []
    
    # Parse JSON format
    for match in json_matches:
        try:
            function_calls = json.loads(match)
            all_calls.extend(function_calls)
        except json.JSONDecodeError:
            continue
    
    # Parse function format
    for func_name, args_str in func_matches:
        if func_name in ["search_internet", "retrieve_clinical_guidelines"]:
            # Simple argument parsing for function format
            call = {"name": func_name, "arguments": {}}
            
            # Extract query parameter
            if "query=" in args_str:
                query_match = re.search(r'query="([^"]*)"', args_str)
                if query_match:
                    call["arguments"]["query"] = query_match.group(1)
            
            # Extract other parameters as needed
            if "num_results=" in args_str:
                num_match = re.search(r'num_results=(\d+)', args_str)
                if num_match:
                    call["arguments"]["num_results"] = int(num_match.group(1))
            
            if "condition=" in args_str:
                condition_match = re.search(r'condition="([^"]*)"', args_str)
                if condition_match:
                    call["arguments"]["condition"] = condition_match.group(1)
            
            if "guideline_source=" in args_str:
                source_match = re.search(r'guideline_source="([^"]*)"', args_str)
                if source_match:
                    call["arguments"]["guideline_source"] = source_match.group(1)
            
            all_calls.append(call)
    
    for call in all_calls:
        function_name = call.get("name")
        arguments = call.get("arguments", {})
        
        # Ask user for permission to access tools
        if function_name == "search_internet":
            query = arguments.get("query", "")
            print(f"üîç I need to search the internet for: '{query}'")
            print("   This will access medical research databases and clinical resources.")
            source_urls.extend([
                "https://www.ahajournals.org/doi/10.1161/HYP.0000000000000217",
                "https://www.nice.org.uk/guidance/ng136",
                "https://academic.oup.com/eurheartj/article/44/31/2971/7243215"
            ])
            
        elif function_name == "retrieve_clinical_guidelines":
            condition = arguments.get("condition", "")
            source = arguments.get("guideline_source", "general")
            if source.upper() == "NICE":
                url = "https://www.nice.org.uk/guidance"
                print(f"üè• I need to check the website: {url}")
                print(f"   To retrieve {source} clinical guidelines for {condition}")
                source_urls.append(url)
            elif source.upper() == "AHA":
                url = "https://www.ahajournals.org/guidelines"
                print(f"üè• I need to check the website: {url}")
                print(f"   To retrieve {source} clinical guidelines for {condition}")
                source_urls.append(url)
            else:
                url = "https://www.who.int/publications/guidelines"
                print(f"üè• I need to check the website: {url}")
                print(f"   To retrieve clinical guidelines for {condition}")
                source_urls.append(url)
        
        print("   Proceeding with search...\n")
        
        print(f"üîß Executing: {function_name}({arguments})")
        
        if function_name == "search_internet":
            # Mock internet search (in real app, use actual search API)
            query = arguments.get("query", "")
            result = f"""Recent search results for '{query}':
1. American Heart Association 2023 Guidelines: New BP targets <130/80 for most adults
2. NICE Guidelines (Updated 2022): ACE inhibitors or ARBs as first-line therapy
3. ESC/ESH 2023 Consensus: Emphasis on lifestyle modifications before medication
4. Recent meta-analysis shows combination therapy reduces cardiovascular events by 25%
5. 2023 studies highlight importance of home BP monitoring for treatment decisions"""
            
        elif function_name == "retrieve_clinical_guidelines":
            condition = arguments.get("condition", "")
            source = arguments.get("guideline_source", "general")
            result = f"""Clinical Guidelines for {condition} from {source}:

NICE Guidelines for Hypertension (2023 Update):
‚Ä¢ Blood pressure targets: <140/90 mmHg for most patients, <130/80 for high CVD risk
‚Ä¢ First-line treatment: ACE inhibitor or ARB for patients <55 years or diabetes
‚Ä¢ For patients ‚â•55 years or African/Caribbean: Calcium channel blocker first-line
‚Ä¢ Step 2: Add calcium channel blocker or thiazide-like diuretic
‚Ä¢ Step 3: Triple therapy with ACE inhibitor/ARB + CCB + diuretic
‚Ä¢ Step 4: Add spironolactone or higher dose diuretic
‚Ä¢ Lifestyle: <6g salt/day, BMI 20-25, regular exercise, limit alcohol
‚Ä¢ Annual review with BP monitoring and cardiovascular risk assessment"""
        
        else:
            result = f"Function {function_name} not implemented in this demo"
        
        function_results.append(result)
        print(f"üìã Result: {result[:100]}...")
    
    # Now generate a follow-up response with the function results
    if function_results:
        print("\nü§ñ Generating final response with search results...\n")
        
        # Create a new prompt with the function results
        results_text = "\n\n".join([f"Search Result {i+1}:\n{result}" for i, result in enumerate(function_results)])
        
        follow_up_messages = messages + [
            {"role": "assistant", "content": assistant_response.strip()},
            {"role": "system", "content": f"Here are the results from your function calls:\n\n{results_text}\n\nNow provide a comprehensive answer to the user's question based on these results. Include the medical disclaimer at the end."},
            {"role": "user", "content": "Please provide your final answer based on the search results."}
        ]
        
        follow_up_prompt = inference_tokenizer.apply_chat_template(
            follow_up_messages, 
            tools=tools,
            add_generation_prompt=True,
            tokenize=False
        )
        
        # Generate final response
        follow_up_inputs = inference_tokenizer(
            follow_up_prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
        )
        follow_up_tensor = follow_up_inputs.input_ids.to(inference_model.device)
        follow_up_mask = follow_up_inputs.attention_mask.to(inference_model.device)
        
        final_gen = inference_model.generate(
            follow_up_tensor,
            attention_mask=follow_up_mask,
            max_new_tokens=400,
            do_sample=True,
            temperature=0.3,
            pad_token_id=inference_tokenizer.pad_token_id,
        )
        
        final_response = inference_tokenizer.decode(final_gen[0], skip_special_tokens=False)
        
        # Extract final assistant response
        if "<|assistant|>" in final_response:
            final_assistant_response = final_response.split("<|assistant|>")[-1]
            if "<|end|>" in final_assistant_response:
                final_assistant_response = final_assistant_response.split("<|end|>")[0]
            
            print("üéØ Final Response with Search Results:")
            print("=" * 60)
            print(final_assistant_response.strip())
            
            # Add source URLs if any were used
            if source_urls:
                print("\nüìö Sources:")
                for i, url in enumerate(set(source_urls), 1):
                    print(f"{i}. {url}")
            
            print("=" * 60)
    
else:
    print("\nüìù Model provided direct answer without tool usage")

# %%



